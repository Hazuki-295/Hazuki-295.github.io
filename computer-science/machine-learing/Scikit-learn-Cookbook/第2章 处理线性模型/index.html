<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="//cdn.jsdelivr.net/gh/Hazuki-295/Hazuki-295.github.io@v1.0.2/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="//cdn.jsdelivr.net/gh/Hazuki-295/Hazuki-295.github.io@v1.0.2/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" href="https://hazuki.cn/rss.xml"><link rel="alternate" type="application/atom+xml" href="https://hazuki.cn/atom.xml"><link rel="alternate" type="application/json" href="https://hazuki.cn/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/Hazuki-295/Hazuki-295.github.io@v1.0.2/css/app.css?v=0.2.5"><meta name="keywords" content="机器学习,Scikit-learn Cookbook"><link rel="canonical" href="https://hazuki.cn/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC2%E7%AB%A0%20%E5%A4%84%E7%90%86%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"><title>第 2 章 处理线性模型 - Scikit-learn Cookbook - 机器学习 - 计算机科学 | Hazuki の 小屋 = = 葉月書架</title><meta name="generator" content="Hexo 5.4.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">第 2 章 处理线性模型</h1><div class="meta"><span class="item" title="创建时间：2021-12-03 18:44:50"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2021-12-03T18:44:50+08:00">2021-12-03</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>25k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>22 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Hazuki の 小屋</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1giclip4jbpj20zk0m87cv.jpg"></li><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1giclwuom7cj20zk0m8dvn.jpg"></li><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1giciszlczyj20zk0m816d.jpg"></li><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1giph4lm9i7j20zk0m84qp.jpg"></li><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1gipewf5l51j20zk0m8b29.jpg"></li><li class="item" data-background-image="https://tva3.sinaimg.cn/large/6833939bly1giclize41wj20zk0m87gk.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/computer-science/" itemprop="item" rel="index" title="分类于 计算机科学"><span itemprop="name">计算机科学</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/computer-science/machine-learing/" itemprop="item" rel="index" title="分类于 机器学习"><span itemprop="name">机器学习</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" itemprop="item" rel="index" title="分类于 Scikit-learn Cookbook"><span itemprop="name">Scikit-learn Cookbook</span></a><meta itemprop="position" content="3"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://hazuki.cn/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC2%E7%AB%A0%20%E5%A4%84%E7%90%86%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="//cdn.jsdelivr.net/gh/Hazuki-295/Hazuki-295.github.io@v1.0.2/images/avatar.jpg"><meta itemprop="name" content="Hazuki 叶月"><meta itemprop="description" content="葉月書架, 计算机基础 & 编程笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content=""></span><div class="body md" itemprop="articleBody"><div class="note info"><p>本文转载自：<span class="exturl" data-url="aHR0cHM6Ly9naXRlZS5jb20vd2l6YXJkZm9yY2VsL3NrbGVhcm4tY2I=">https://gitee.com/wizardforcel/sklearn-cb</span><br>未经作者的许可，此代码仅用于学习，不能用于其他用途。</p></div><h1 id="第二章-处理线性模型"><a class="anchor" href="#第二章-处理线性模型">#</a> 第二章 处理线性模型</h1><h2 id="简介"><a class="anchor" href="#简介">#</a> 简介</h2><div class="note primary"><p><code>线性模型</code> 是统计学和机器学习的基础。很多方法都利用变量的 <code>线性组合</code> 描述数据之间的关系。通常都要花费很大精力做各种变换，目的就是为了让数据可以描述成一种线性组合形式。</p></div><p>本章，我们将从最简单的 <code>数据直线拟合模型</code> 到 <code>分类模型</code> ，最后介绍 <code>贝叶斯岭回归</code> 。</p><h1 id="线性回归模型"><a class="anchor" href="#线性回归模型">#</a> 线性回归模型</h1><p>现在，我们来做一些建模！我们从最简单的 <code>线性回归（Linear regression）</code> 开始。线性回归是最早的也是最基本的模型：把数据拟合成一条直线。</p><h2 id="getting-ready"><a class="anchor" href="#getting-ready">#</a> Getting ready</h2><p><code>boston</code> 数据集很适合用来演示线性回归。 <code>boston</code> 数据集包含了波士顿地区的房屋价格中位数。还有一些可能会影响房价的因素，比如犯罪率（crime rate）。</p><p>首先，让我们加载数据：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre>boston <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h2 id="how-to-do-it"><a class="anchor" href="#how-to-do-it">#</a> How to do it...</h2><p>实际上，用 scikit-learn 的线性回归非常简单，其 API 和前面介绍的模型一样。</p><p>首先，导入 <code>LinearRegression</code> 类创建一个对象：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegression</pre></td></tr><tr><td data-num="2"></td><td><pre>lr <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>现在，再把自变量和因变量传给 <code>LinearRegression</code> 的 <code>fit</code> 方法：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>lr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>data<span class="token punctuation">,</span> boston<span class="token punctuation">.</span>target<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</code></pre><h1 id="评估线性回归模型"><a class="anchor" href="#评估线性回归模型">#</a> 评估线性回归模型</h1><p>在这个主题中，我们将介绍回归模型 <code>拟合数据的效果</code> 。上一个主题我们拟合了数据，但是并没太关注拟合的效果。每当拟合工作做完之后，我们应该问的第一个问题就是 “拟合的效果如何？” 本主题将回答这个问题。</p><h2 id="getting-ready-2"><a class="anchor" href="#getting-ready-2">#</a> Getting ready</h2><p>我们还用上一主题里的 <code>lr</code> 对象和 <code>boston</code> 数据集。 <code>lr</code> 对象已经拟合过数据，现在有许多方法可以用。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre>boston <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegression</pre></td></tr><tr><td data-num="4"></td><td><pre>lr <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>lr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>data<span class="token punctuation">,</span> boston<span class="token punctuation">.</span>target<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>predictions <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>data<span class="token punctuation">)</span></pre></td></tr></table></figure><h2 id="how-to-do-it-2"><a class="anchor" href="#how-to-do-it-2">#</a> How to do it...</h2><p>我们可以看到一些简单的 <code>量度（metris）</code> 和图形。让我们看看上一章的 <code>残差图</code> ：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="3"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>f<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>target <span class="token operator">-</span> predictions<span class="token punctuation">,</span> bins<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Residuals Linear'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">.5</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Histogram of Residuals"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'best'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-2-1.png" alt="png"></p><p>如果你用 IPython Notebook，就用 <code>%matplotlib inline</code> 命令在网页中显示 matplotlib 图形。如果你不用，就用 <code>f.savefig('myfig.png')</code> 保存图形，以备使用。</p><div class="note success"><p>画图的库是<span class="exturl" data-url="aHR0cDovL21hdHBsb3RsaWIub3JnLw=="> matplotlib</span>，并非本书重点，但是可视化效果非常好。</p></div><p>和之前介绍的一样， <code>误差项</code> 服从均值为 0 的正态分布。残差就是误差，所以这个图也应该近似正态分布。看起来拟合挺好的，只是有点偏。我们计算一下残差的均值，应该很接近 0：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre>np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>target <span class="token operator">-</span> predictions<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>6.0382090193051989e-16
</code></pre><p>另一个值得看的图是 <code>Q-Q图（分位数概率分布）</code> ，我们用 <code>Scipy</code> 来实现图形，因为它内置这个概率分布图的方法：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> probplot</pre></td></tr><tr><td data-num="2"></td><td><pre>f <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>ax <span class="token operator">=</span> f<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>probplot<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>target <span class="token operator">-</span> predictions<span class="token punctuation">,</span> plot<span class="token operator">=</span>ax<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-2-2.png" alt="png"></p><p>这个图里面倾斜的数据比之前看的要更清楚一些。</p><p>我们还可以观察拟合其他量度，最常用的还有 <code>均方误差</code> （mean squared error， <code>MSE</code> ）， <code>平均绝对误差</code> （mean absolute deviation， <code>MAD</code> ）。让我们用 Python 实现这两个量度。后面我们用 scikit-learn 内置的量度来评估回归模型的效果：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">MSE</span><span class="token punctuation">(</span>target<span class="token punctuation">,</span> predictions<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    squared_deviation <span class="token operator">=</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>target <span class="token operator">-</span> predictions<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>squared_deviation<span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>MSE<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>target<span class="token punctuation">,</span> predictions<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>21.897779217687496
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">MAD</span><span class="token punctuation">(</span>target<span class="token punctuation">,</span> predictions<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    absolute_deviation <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>target <span class="token operator">-</span> predictions<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>absolute_deviation<span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>MAD<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>target<span class="token punctuation">,</span> predictions<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>3.2729446379969396
</code></pre><h2 id="how-it-works"><a class="anchor" href="#how-it-works">#</a> How it works...</h2><p><code>MSE</code> 的计算公式是：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>t</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">E(\hat y_t - y_i)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05764em">E</span><span class="mopen">(</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.19444em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.1141079999999999em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8641079999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p><p>计算预测值与实际值的差，平方之后再求平均值。这其实就是我们寻找 <code>最佳相关系数</code> 时是目标。高斯－马尔可夫定理（Gauss-Markov theorem）实际上已经证明了线性回归的回归系数的 <code>最佳线性无偏估计（BLUE）</code> 就是最小均方误差的无偏估计（条件是误差变量不相关，0 均值，同方差）。在<span class="blue">用岭回归弥补线性回归的不足</span>主题中，我们会看到，当我们的相关系数是有偏估计时会发生什么。</p><p><code>MAD</code> 是平均绝对误差，计算公式为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>E</mi><mi mathvariant="normal">∣</mi><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>t</mi></msub><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">E|\hat y_t - y_i|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05764em">E</span><span class="mord">∣</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.19444em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-.03588em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mord">∣</span></span></span></span></span></p><p>线性回归的时候 MAD 通常不用，但是值得一看。为什么呢？可以看到每个量度的情况，还可以判断哪个量度更重要。例如，用 MSE，较大的误差会获得更大的惩罚，因为平方把它放大。</p><h3 id="theres-more"><a class="anchor" href="#theres-more">#</a> There's more...</h3><p>还有一点需要说明，那就是相关系数是 <code>随机变量</code> ，因此它们是有分布的。让我们用 <code>bootstrapping（重复试验）</code> 来看看犯罪率的相关系数的分布情况。bootstrapping 是一种学习 <code>参数估计不确定性</code> 的常用手段：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>n_bootstraps <span class="token operator">=</span> <span class="token number">1000</span></pre></td></tr><tr><td data-num="2"></td><td><pre>len_boston <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>boston<span class="token punctuation">.</span>target<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>subsample_size <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token operator">*</span>len_boston<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>subsample <span class="token operator">=</span> <span class="token keyword">lambda</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> len_boston<span class="token punctuation">)</span><span class="token punctuation">,</span>size<span class="token operator">=</span>subsample_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>coefs <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>n_bootstraps<span class="token punctuation">)</span> <span class="token comment">#相关系数初始值设为 1</span></pre></td></tr><tr><td data-num="6"></td><td><pre><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_bootstraps<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    subsample_idx <span class="token operator">=</span> subsample<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    subsample_X <span class="token operator">=</span> boston<span class="token punctuation">.</span>data<span class="token punctuation">[</span>subsample_idx<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    subsample_y <span class="token operator">=</span> boston<span class="token punctuation">.</span>target<span class="token punctuation">[</span>subsample_idx<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    lr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>subsample_X<span class="token punctuation">,</span> subsample_y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>    coefs<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> lr<span class="token punctuation">.</span>coef_<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr></table></figure><p>我们可以看到这个相关系数的分布直方图：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>f <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>ax <span class="token operator">=</span> f<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>ax<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>coefs<span class="token punctuation">,</span> bins<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">.5</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Histogram of the lr.coef_[0]."</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-2-3.png" alt="png"></p><p>我们还想看看重复试验后的置信区间：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>percentile<span class="token punctuation">(</span>coefs<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.5</span><span class="token punctuation">,</span> <span class="token number">97.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([-0.18030624,  0.03816062])
</code></pre><p>置信区间的范围表明犯罪率其实不影响房价，因为 0 在置信区间里面，表面犯罪率可能与房价无关。</p><p>值得一提的是，bootstrapping 可以获得更好的相关系数估计值，因为使用 bootstrapping 方法的均值，会比普通估计方法更快地<span class="aqua">收敛（converge）</span>到真实均值。</p><h1 id="用岭回归弥补线性回归的不足"><a class="anchor" href="#用岭回归弥补线性回归的不足">#</a> 用岭回归弥补线性回归的不足</h1><p>本主题将介绍岭回归。和线性回归不同，它引入了正则化参数来 “缩减” 相关系数。当数据集中存在共线因素时，岭回归会很有用。</p><h2 id="getting-ready-3"><a class="anchor" href="#getting-ready-3">#</a> Getting ready</h2><p>让我们加载一个不满秩（low effective rank）数据集来比较岭回归和线性回归。秩是矩阵线性无关组的数量，满秩是指一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span> 矩阵中行向量或列向量中现行无关组的数量等于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">min(m,n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span>。</p><h2 id="how-to-do-it-3"><a class="anchor" href="#how-to-do-it-3">#</a> How to do it...</h2><p>首先我们用 <code>make_regression</code> 建一个有 3 个自变量的数据集，但是其秩为 2，因此 3 个自变量中有两个自变量存在相关性。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_regression</pre></td></tr><tr><td data-num="2"></td><td><pre>reg_data<span class="token punctuation">,</span> reg_target <span class="token operator">=</span> make_regression<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">2000</span><span class="token punctuation">,</span> n_features<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> effective_rank<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> noise<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>首先，我们用普通的线性回归拟合：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegression</pre></td></tr><tr><td data-num="3"></td><td><pre>lr <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre></pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">def</span> <span class="token function">fit_2_regression</span><span class="token punctuation">(</span>lr<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    n_bootstraps <span class="token operator">=</span> <span class="token number">1000</span></pre></td></tr><tr><td data-num="7"></td><td><pre>    coefs <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>n_bootstraps<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>    len_data <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>reg_data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>    subsample_size <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">0.75</span><span class="token operator">*</span>len_data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>    subsample <span class="token operator">=</span> <span class="token keyword">lambda</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> len_data<span class="token punctuation">)</span><span class="token punctuation">,</span> size<span class="token operator">=</span>subsample_size<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre></pre></td></tr><tr><td data-num="12"></td><td><pre>    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_bootstraps<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="13"></td><td><pre>        subsample_idx <span class="token operator">=</span> subsample<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre>        subsample_X <span class="token operator">=</span> reg_data<span class="token punctuation">[</span>subsample_idx<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="15"></td><td><pre>        subsample_y <span class="token operator">=</span> reg_target<span class="token punctuation">[</span>subsample_idx<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="16"></td><td><pre>        lr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>subsample_X<span class="token punctuation">,</span> subsample_y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="17"></td><td><pre>        coefs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> lr<span class="token punctuation">.</span>coef_<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="18"></td><td><pre>        coefs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> lr<span class="token punctuation">.</span>coef_<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="19"></td><td><pre>        coefs<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> lr<span class="token punctuation">.</span>coef_<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="20"></td><td><pre>    <span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="21"></td><td><pre>    <span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="22"></td><td><pre>    f<span class="token punctuation">,</span> axes <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>nrows<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> sharey<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> sharex<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="23"></td><td><pre>    f<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="24"></td><td><pre></pre></td></tr><tr><td data-num="25"></td><td><pre>    <span class="token keyword">for</span> i<span class="token punctuation">,</span> ax <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>axes<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="26"></td><td><pre>        ax<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>coefs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">.5</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="27"></td><td><pre>        ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Coef &#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="28"></td><td><pre>    <span class="token keyword">return</span> coefs</pre></td></tr><tr><td data-num="29"></td><td><pre></pre></td></tr><tr><td data-num="30"></td><td><pre>coefs <span class="token operator">=</span> fit_2_regression<span class="token punctuation">(</span>lr<span class="token punctuation">)</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-3-1.png" alt="png"></p><p>我们再用 <code>Ridge</code> 来拟合数据，对比结果：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> Ridge</pre></td></tr><tr><td data-num="2"></td><td><pre>coefs_r <span class="token operator">=</span> fit_2_regression<span class="token punctuation">(</span>Ridge<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-3-2.png" alt="png"></p><p>两个回归算法的结果看着好像差不多，其实不然。岭回归的相关系数更接近 0。让我们看看两者相关系数的差异：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>coefs <span class="token operator">-</span> coefs_r<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([ 30.54319761,  25.1726559 ,   7.40345307])
</code></pre><p>从均值上看，线性回归比岭回归的相关系数要更很多。均值显示的差异其实是线性回归的相关系数隐含的偏差。那么，岭回归究竟有什么好处呢？让我们再看看相关系数的方差：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>var<span class="token punctuation">(</span>coefs<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([ 302.16242654,  177.36842779,  179.33610289])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>var<span class="token punctuation">(</span>coefs_r<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([ 19.60727206,  25.4807605 ,  22.74202917])
</code></pre><p>岭回归的相关系数方差也会小很多。这就是机器学习里著名的偏差 - 方差均衡 (Bias-Variance Trade-off)。下一个主题我们将介绍如何调整岭回归的参数正则化，那是偏差 - 方差均衡的核心内容。</p><h2 id="how-it-works-2"><a class="anchor" href="#how-it-works-2">#</a> How it works...</h2><p>介绍参数正则化之前，我们总结一下岭回归与线性回归的不同。前面介绍过，线性回归的目标是最小化<br><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo fence="true">∥</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>−</mo><mi>X</mi><mi>β</mi></mrow></mstyle></mtd></mtr></mtable><mo fence="true">∥</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">{\begin{Vmatrix} \hat y - X \beta \end{Vmatrix}}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.421998em;vertical-align:-.35000999999999993em"></span><span class="mord"><span class="mord"><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8500000000000001em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.19444em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000000000000003em"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0719880000000002em"><span style="top:-3.3208800000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>。</p><p>岭回归的目标是最小化<br><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow><mo fence="true">∥</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>−</mo><mi>X</mi><mi>β</mi></mrow></mstyle></mtd></mtr></mtable><mo fence="true">∥</mo></mrow><mn>2</mn></msup><mo>+</mo><msup><mrow><mo fence="true">∥</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">Γ</mi><mi>X</mi></mrow></mstyle></mtd></mtr></mtable><mo fence="true">∥</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">{\begin{Vmatrix} \hat y - X \beta \end{Vmatrix}}^2 + {\begin{Vmatrix} \Gamma X \end{Vmatrix}}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.421998em;vertical-align:-.35000999999999993em"></span><span class="mord"><span class="mord"><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8500000000000001em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.19444em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000000000000003em"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0719880000000002em"><span style="top:-3.3208800000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.421998em;vertical-align:-.35000999999999993em"></span><span class="mord"><span class="mord"><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8500000000000001em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">Γ</span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000000000000003em"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0719880000000002em"><span style="top:-3.3208800000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>。</p><p>其中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Γ</mi></mrow><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord">Γ</span></span></span></span> 就是岭回归 <code>Ridge</code> 的 <code>alpha</code> 参数，指单位矩阵的倍数。上面的例子用的是默认值。我们可以看看岭回归参数：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>Ridge<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
   normalize=False, solver='auto', tol=0.001)
</code></pre><p>岭回归相关系数的解是：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>β</mi><mo>=</mo><msup><mrow><mo stretchy="false">(</mo><msup><mi>X</mi><mi>T</mi></msup><mi>X</mi><mo>+</mo><msup><mi mathvariant="normal">Γ</mi><mi>T</mi></msup><mi mathvariant="normal">Γ</mi><mo stretchy="false">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>X</mi><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\beta = {(X^TX + \Gamma ^ T \Gamma)}^{-1}X \hat y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.05278em">β</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.3453389999999998em;vertical-align:-.25em"></span><span class="mord"><span class="mord"><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8913309999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mord">Γ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8913309999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span><span class="mord">Γ</span><span class="mclose">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.0953389999999998em"><span style="top:-3.3442309999999997em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.19444em"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.19444em"><span></span></span></span></span></span></span></span></span></span></p><p>前面的一半和线性回归的相关系数的解是一样的，多了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="normal">Γ</mi><mi>T</mi></msup><mi mathvariant="normal">Γ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\Gamma ^ T \Gamma)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-.25em"></span><span class="mord"><span class="mord">Γ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span><span class="mord">Γ</span><span class="mclose">)</span></span></span></span> 一项。矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal">A</span></span></span></span> 的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><msup><mi>A</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">AA^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8413309999999999em;vertical-align:0"></span><span class="mord mathnormal">A</span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span></span></span></span> 的结果是对称矩阵，且是半正定矩阵（对任意非 0 向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span>，有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mi>T</mi></msup><mi>A</mi><mi>x</mi><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">x^TAx \ge 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.977301em;vertical-align:-.13597em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal">A</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span></span></span></span>）。相当于在线性回归的目标函数分母部分增加了一个很大的数。这样就把相关系数挤向 0 了。这样的解释比较粗糙，要深入了解，建议你看看 SVD（矩阵奇异值分解）与岭回归的关系。</p><h1 id="优化岭回归参数"><a class="anchor" href="#优化岭回归参数">#</a> 优化岭回归参数</h1><p>当你使用岭回归模型进行建模时，需要考虑 <code>Ridge</code> 的 <code>alpha</code> 参数。</p><p>例如，用 OLS（普通最小二乘法）做回归也许可以显示两个变量之间的某些关系；但是，当 <code>alpha</code> 参数正则化之后，那些关系就会消失。做决策时，这些关系是否需要考虑就显得很重要了。</p><h2 id="getting-ready-4"><a class="anchor" href="#getting-ready-4">#</a> Getting ready</h2><p>这是我们第一个进行模型参数优化的主题，通常用交叉检验（cross validation）完成。在后面的主题中，还会有更简便的方式实现这些，但是这里我们一步一步来实现岭回归的优化。</p><p>在 scikit-learn 里面，岭回归的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Γ</mi></mrow><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord">Γ</span></span></span></span> 参数就是 <code>RidgeRegression</code> 的 <code>alpha</code> 参数；因此，问题就是最优的 <code>alpha</code> 参数是什么。首先我们建立回归数据集：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_regression</pre></td></tr><tr><td data-num="2"></td><td><pre>reg_data<span class="token punctuation">,</span> reg_target <span class="token operator">=</span> make_regression<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> n_features<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> effective_rank<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> noise<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h2 id="how-to-do-it-4"><a class="anchor" href="#how-to-do-it-4">#</a> How to do it...</h2><p>在 <code>linear_models</code> 模块中，有一个对象叫 <code>RidgeCV</code> ，表示<strong>岭回归交叉检验（ridge cross-validation）</strong>。这个交叉检验类似于<strong>留一交叉验证法（leave-one-out cross-validation，LOOCV）</strong>。这种方法是指训练数据时留一个样本，测试的时候用这个未被训练过的样本：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> RidgeCV</pre></td></tr><tr><td data-num="3"></td><td><pre>rcv <span class="token operator">=</span> RidgeCV<span class="token punctuation">(</span>alphas<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">.1</span><span class="token punctuation">,</span> <span class="token number">.2</span><span class="token punctuation">,</span> <span class="token number">.3</span><span class="token punctuation">,</span> <span class="token number">.4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>rcv<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>reg_data<span class="token punctuation">,</span> reg_target<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>RidgeCV(alphas=array([ 0.1,  0.2,  0.3,  0.4]), cv=None, fit_intercept=True,
    gcv_mode=None, normalize=False, scoring=None, store_cv_values=False)
</code></pre><p>拟合模型之后， <code>alpha</code> 参数就是最优参数：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>rcv<span class="token punctuation">.</span>alpha_</pre></td></tr></table></figure><pre><code>0.10000000000000001
</code></pre><p>这里， <code>0.1</code> 是最优参数，我们还想看到 <code>0.1</code> 附近更精确的值：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>rcv <span class="token operator">=</span> RidgeCV<span class="token punctuation">(</span>alphas<span class="token operator">=</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">.08</span><span class="token punctuation">,</span> <span class="token number">.09</span><span class="token punctuation">,</span> <span class="token number">.1</span><span class="token punctuation">,</span> <span class="token number">.11</span><span class="token punctuation">,</span> <span class="token number">.12</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>rcv<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>reg_data<span class="token punctuation">,</span> reg_target<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>RidgeCV(alphas=array([ 0.08,  0.09,  0.1 ,  0.11,  0.12]), cv=None,
    fit_intercept=True, gcv_mode=None, normalize=False, scoring=None,
    store_cv_values=False)
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>rcv<span class="token punctuation">.</span>alpha_</pre></td></tr></table></figure><pre><code>0.080000000000000002
</code></pre><p>可以按照这个思路一直优化下去，这里只做演示，后面还是介绍更好的方法。</p><h2 id="how-it-works-3"><a class="anchor" href="#how-it-works-3">#</a> How it works...</h2><p>上面的演示很直接，但是我们介绍一下为什么这么做，以及哪个值才是最优的。在交叉检验的每一步里，模型的拟合效果都是用测试样本的误差表示。默认情况使用平方误差。更多细节见 <code>There's more...</code> 一节。</p><p>我们可以让 <code>RidgeCV</code> 储存交叉检验的数据，这样就可以可视化整个过程：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>alphas_to_test <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.0001</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>rcv3 <span class="token operator">=</span> RidgeCV<span class="token punctuation">(</span>alphas<span class="token operator">=</span>alphas_to_test<span class="token punctuation">,</span> store_cv_values<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>rcv3<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>reg_data<span class="token punctuation">,</span> reg_target<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>RidgeCV(alphas=array([ 0.0001 ,  0.00112,  0.00214,  0.00316,  0.00417,  0.00519,
        0.00621,  0.00723,  0.00825,  0.00927,  0.01028,  0.0113 ,
        0.01232,  0.01334,  0.01436,  0.01538,  0.01639,  0.01741,
        0.01843,  0.01945,  0.02047,  0.02149,  0.0225 ,  0.02352,
        0.02454,  0.02556...4185,
        0.04287,  0.04389,  0.04491,  0.04593,  0.04694,  0.04796,
        0.04898,  0.05   ]),
    cv=None, fit_intercept=True, gcv_mode=None, normalize=False,
    scoring=None, store_cv_values=True)
</code></pre><p>你会看到，我们测试了 0.0001 到 0.05 区间中的 50 个点。由于我们把 <code>store_cv_values</code> 设置成 <code>true</code> ，我们可以看到每一个值对应的拟合效果：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>rcv3<span class="token punctuation">.</span>cv_values_<span class="token punctuation">.</span>shape</pre></td></tr></table></figure><pre><code>(100, 50)
</code></pre><p>通过 100 个样本的回归数据集，我们获得了 50 个不同的 <code>alpha</code> 值。我们可以看到 50 个误差值，最小的均值误差对应最优的 <code>alpha</code> 值：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>smallest_idx <span class="token operator">=</span> rcv3<span class="token punctuation">.</span>cv_values_<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argmin<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>alphas_to_test<span class="token punctuation">[</span>smallest_idx<span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>0.014357142857142857
</code></pre><p>此时问题转化成了 “RidgeCV 认可我们的选择吗？” 可以再用下面的命令获取 <code>alpha</code> 值：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>rcv3<span class="token punctuation">.</span>alpha_</pre></td></tr></table></figure><pre><code>0.014357142857142857
</code></pre><p>通过可视化图形可以更直观的显示出来。我们画出 50 个测试 <code>alpha</code> 值的图：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="3"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">r"Various values of $\alpha$"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>xy <span class="token operator">=</span> <span class="token punctuation">(</span>alphas_to_test<span class="token punctuation">[</span>smallest_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> rcv3<span class="token punctuation">.</span>cv_values_<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span>smallest_idx<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>xytext <span class="token operator">=</span> <span class="token punctuation">(</span>xy<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">.01</span><span class="token punctuation">,</span> xy<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">.1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre></pre></td></tr><tr><td data-num="9"></td><td><pre>ax<span class="token punctuation">.</span>annotate<span class="token punctuation">(</span><span class="token string">r'Chosen $\alpha$'</span><span class="token punctuation">,</span> xy<span class="token operator">=</span>xy<span class="token punctuation">,</span> xytext<span class="token operator">=</span>xytext<span class="token punctuation">,</span></pre></td></tr><tr><td data-num="10"></td><td><pre>            arrowprops<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> shrink<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> width<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>            <span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>alphas_to_test<span class="token punctuation">,</span> rcv3<span class="token punctuation">.</span>cv_values_<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-4-1.png" alt="png"></p><h2 id="theres-more-2"><a class="anchor" href="#theres-more-2">#</a> There's more...</h2><p>如果我们想用其他误差自定义评分函数，也是可以实现的。前面我们介绍过 MAD 误差，我们可以用它来评分。首先我们需要定义损失函数：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">def</span> <span class="token function">MAD</span><span class="token punctuation">(</span>target<span class="token punctuation">,</span> prediction<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="2"></td><td><pre>    absolute_deviation <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span>target <span class="token operator">-</span> prediction<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>    <span class="token keyword">return</span> absolute_deviation<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>定义损失函数之后，我们用 <code>sklearn</code> 量度中的 <code>make_scorer</code> 函数来处理。这样做可以标准化自定义的函数，让 scikit-learn 对象可以使用它。另外，由于这是一个损失函数不是一个评分函数，是越低越好，所以要用 <code>sklearn</code> 来把最小化问题转化成最大化问题：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> sklearn</pre></td></tr><tr><td data-num="2"></td><td><pre>MAD <span class="token operator">=</span> sklearn<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>make_scorer<span class="token punctuation">(</span>MAD<span class="token punctuation">,</span> greater_is_better<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>rcv4 <span class="token operator">=</span> RidgeCV<span class="token punctuation">(</span>alphas<span class="token operator">=</span>alphas_to_test<span class="token punctuation">,</span> store_cv_values<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> scoring<span class="token operator">=</span>MAD<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>rcv4<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>reg_data<span class="token punctuation">,</span> reg_target<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>smallest_idx <span class="token operator">=</span> rcv4<span class="token punctuation">.</span>cv_values_<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>argmin<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>alphas_to_test<span class="token punctuation">[</span>smallest_idx<span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>0.050000000000000003
</code></pre><h1 id="lasso正则化"><a class="anchor" href="#lasso正则化">#</a> LASSO 正则化</h1><p>LASSO（ least absolute shrinkage and selection operator，最小绝对值收缩和选择算子）方法与岭回归和 LARS（least angle regression，最小角回归）很类似。与岭回归类似，它也是通过增加惩罚函数来判断、消除特征间的共线性。与 LARS 相似的是它也可以用作参数选择，通常得出一个相关系数的稀疏向量。</p><h2 id="getting-ready-5"><a class="anchor" href="#getting-ready-5">#</a> Getting ready</h2><p>岭回归也不是万能药。有时就需要用 LASSO 回归来建模。本主题将用不同的损失函数，因此就要用对应的效果评估方法。</p><h2 id="how-to-do-it-5"><a class="anchor" href="#how-to-do-it-5">#</a> How to do it...</h2><p>首先，我们还是用 <code>make_regression</code> 函数来建立数据集：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_regression</pre></td></tr><tr><td data-num="2"></td><td><pre>reg_data<span class="token punctuation">,</span> reg_target <span class="token operator">=</span> make_regression<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span> n_features<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> n_informative<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> noise<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>之后，我们导入 <code>lasso</code> 对象：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> Lasso</pre></td></tr><tr><td data-num="2"></td><td><pre>lasso <span class="token operator">=</span> Lasso<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p><code>lasso</code> 包含很多参数，但是最意思的参数是 <code>alpha</code> ，用来调整 <code>lasso</code> 的惩罚项，在 **How it works...** 会具体介绍。现在我们用默认值 <code>1</code> 。另外，和岭回归类似，如果设置为 <code>0</code> ，那么 <code>lasso</code> 就是线性回归：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>lasso<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>reg_data<span class="token punctuation">,</span> reg_target<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection='cyclic', tol=0.0001, warm_start=False)
</code></pre><p>再让我们看看还有多少相关系数非零：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>lasso<span class="token punctuation">.</span>coef_ <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>9
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>lasso_0 <span class="token operator">=</span> Lasso<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>lasso_0<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>reg_data<span class="token punctuation">,</span> reg_target<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>lasso_0<span class="token punctuation">.</span>coef_ <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>d:\programfiles\Miniconda3\lib\site-packages\IPython\kernel\__main__.py:2: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator
  from IPython.kernel.zmq import kernelapp as app
d:\programfiles\Miniconda3\lib\site-packages\sklearn\linear_model\coordinate_descent.py:432: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.
  positive)





500
</code></pre><p>和我们设想的一样，如果用线性回归，没有一个相关系数变成 0。而且，如果你这么运行代码，scikit-learn 会给出建议，就像上面显示的那样。</p><h2 id="how-it-works-4"><a class="anchor" href="#how-it-works-4">#</a> How it works...</h2><p>对线性回归来说，我们是最小化残差平方和。而 LASSO 回归里，我们还是最小化残差平方和，但是加了一个惩罚项会导致稀疏。如下所示：</p>\sum {e_i + \lambda \ {\begin{Vmatrix} \beta \end{Vmatrix}}_1}<p>最小化残差平方和的另一种表达方式是：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>R</mi><mi>S</mi><mi>S</mi><mo stretchy="false">(</mo><mi>β</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mtext>其中</mtext><msub><mrow><mo fence="true">∥</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>β</mi></mstyle></mtd></mtr></mtable><mo fence="true">∥</mo></mrow><mn>1</mn></msub><mo>&lt;</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">RSS(\beta),其中 {\begin{Vmatrix} \beta \end{Vmatrix}}_1 \lt \beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.26769em;vertical-align:-.3997099999999999em"></span><span class="mord mathnormal" style="margin-right:.00773em">R</span><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="mord mathnormal" style="margin-right:.05764em">S</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.05278em">β</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord cjk_fallback">其</span><span class="mord cjk_fallback">中</span><span class="mord"><span class="mord"><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8500000000000001em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000000000000003em"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.051398000000000055em"><span style="top:-2.3002900000000004em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.3997099999999999em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span></span></p><p>这个约束会让数据稀疏。LASSO 回归的约束创建了围绕原点的超立方体（相关系数是轴），也就意味着大多数点都在各个顶点上，那里相关系数为 0。而岭回归创建的是超平面，因为其约束是 L2 范数，少一个约束，但是即使有限制相关系数也不会变成 0。</p><h3 id="lasso交叉检验"><a class="anchor" href="#lasso交叉检验">#</a> LASSO 交叉检验</h3><p>上面的公式中，选择适当的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span>（在 scikit-learn 的 <code>Lasso</code> 里面是 <code>alpha</code> ，但是书上都是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span>）参数是关键。我们可以自己设置，也可以通过交叉检验来获取最优参数：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LassoCV</pre></td></tr><tr><td data-num="2"></td><td><pre>lassocv <span class="token operator">=</span> LassoCV<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>lassocv<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>reg_data<span class="token punctuation">,</span> reg_target<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,
    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,
    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,
    verbose=False)
</code></pre><p><code>lassocv</code> 有一个属性就是确定最合适的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span>：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>lassocv<span class="token punctuation">.</span>alpha_</pre></td></tr></table></figure><pre><code>0.58535963603062136
</code></pre><p>计算的相关系数也可以看到：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>lassocv<span class="token punctuation">.</span>coef_<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([ 0.       , -0.       ,  0.       ,  0.0192606, -0.       ])
</code></pre><p>用最近的参数拟合后， <code>lassocv</code> 的非零相关系数有 29 个：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>lassocv<span class="token punctuation">.</span>coef_ <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>29
</code></pre><h3 id="lasso特征选择"><a class="anchor" href="#lasso特征选择">#</a> LASSO 特征选择</h3><p>LASSO 通常用来为其他方法所特征选择。例如，你可能会用 LASSO 回归获取适当的特征变量，然后在其他算法中使用。</p><p>要获取想要的特征，需要创建一个非零相关系数的列向量，然后再其他算法拟合：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>mask <span class="token operator">=</span> lassocv<span class="token punctuation">.</span>coef_ <span class="token operator">!=</span> <span class="token number">0</span></pre></td></tr><tr><td data-num="2"></td><td><pre>new_reg_data <span class="token operator">=</span> reg_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> mask<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="3"></td><td><pre>new_reg_data<span class="token punctuation">.</span>shape</pre></td></tr></table></figure><pre><code>(200, 29)
</code></pre><h1 id="lars正则化"><a class="anchor" href="#lars正则化">#</a> LARS 正则化</h1><p>如果斯坦福大学的 Bradley Efron, Trevor Hastie, Iain Johnstone 和 Robert Tibshirani 没有发现它的话 [1]，LARS (Least Angle Regression，最小角回归) 可能有一天会被你想出来，它借用了<span class="exturl" data-url="aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvR2lsYmVydF9TdHJhbmc=">威廉・吉尔伯特・斯特朗（William Gilbert Strang）</span>介绍过的高斯消元法（Gaussian elimination）的灵感。</p><h2 id="getting-ready-6"><a class="anchor" href="#getting-ready-6">#</a> Getting ready</h2><p>LARS 是一种回归手段，适用于解决高维问题，也就是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mo>&gt;</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">p &gt;&gt; n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.7335400000000001em;vertical-align:-.19444em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&gt;</span></span><span class="base"><span class="strut" style="height:.5782em;vertical-align:-.0391em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span> 的情况，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal">p</span></span></span></span> 表示列或者特征变量，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span> 表示样本数量。</p><h2 id="how-to-do-it-6"><a class="anchor" href="#how-to-do-it-6">#</a> How to do it...</h2><p>首先让我们导入必要的对象。这里我们用的数据集是 200 个数据，500 个特征。我们还设置了一个低噪声，和少量提供信息的（informative）特征：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_regression</pre></td></tr><tr><td data-num="3"></td><td><pre>reg_data<span class="token punctuation">,</span> reg_target <span class="token operator">=</span> make_regression<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span>n_features<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> n_informative<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> noise<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>由于我们用了 10 个信息特征，因此我们还要为 LARS 设置 10 个非 0 的相关系数。我们事先可能不知道信息特征的准确数量，但是出于试验的目的是可行的：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> Lars</pre></td></tr><tr><td data-num="2"></td><td><pre>lars <span class="token operator">=</span> Lars<span class="token punctuation">(</span>n_nonzero_coefs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>lars<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>reg_data<span class="token punctuation">,</span> reg_target<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>Lars(copy_X=True, eps=2.2204460492503131e-16, fit_intercept=True,
   fit_path=True, n_nonzero_coefs=10, normalize=True, precompute='auto',
   verbose=False)
</code></pre><p>我们可以检验一下看看 LARS 的非 0 相关系数的和：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>lars<span class="token punctuation">.</span>coef_ <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>10
</code></pre><p>问题在于为什么少量的特征反而变得更加有效。要证明这一点，让我们用一半数量来训练两个 LARS 模型，一个用 12 个非零相关系数，另一个非零相关系数用默认值。这里用 12 个是因为我们对重要特征的数量有个估计，但是可能无法确定准确的数量：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>train_n <span class="token operator">=</span> <span class="token number">100</span></pre></td></tr><tr><td data-num="2"></td><td><pre>lars_12 <span class="token operator">=</span> Lars<span class="token punctuation">(</span>n_nonzero_coefs<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>lars_12<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>reg_data<span class="token punctuation">[</span><span class="token punctuation">:</span>train_n<span class="token punctuation">]</span><span class="token punctuation">,</span> reg_target<span class="token punctuation">[</span><span class="token punctuation">:</span>train_n<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>Lars(copy_X=True, eps=2.2204460492503131e-16, fit_intercept=True,
   fit_path=True, n_nonzero_coefs=12, normalize=True, precompute='auto',
   verbose=False)
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>lars_500 <span class="token operator">=</span> Lars<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">#默认就是 500</span></pre></td></tr><tr><td data-num="2"></td><td><pre>lars_500<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>reg_data<span class="token punctuation">[</span><span class="token punctuation">:</span>train_n<span class="token punctuation">]</span><span class="token punctuation">,</span> reg_target<span class="token punctuation">[</span><span class="token punctuation">:</span>train_n<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>Lars(copy_X=True, eps=2.2204460492503131e-16, fit_intercept=True,
   fit_path=True, n_nonzero_coefs=500, normalize=True, precompute='auto',
   verbose=False)
</code></pre><p>现在，让我们看看拟合数据的效果如何，如下所示：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>reg_target<span class="token punctuation">[</span>train_n<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">-</span> lars<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>reg_data<span class="token punctuation">[</span>train_n<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>18.607806437043894
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>reg_target<span class="token punctuation">[</span>train_n<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">-</span> lars_12<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>reg_data<span class="token punctuation">[</span>train_n<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>529.97993250189643
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>reg_target<span class="token punctuation">[</span>train_n<span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">-</span> lars_500<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>reg_data<span class="token punctuation">[</span>train_n<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>2.3236770314162846e+34
</code></pre><p>仔细看看这组结果；测试集的误差明显高很多。高维数据集问题就在于此；通常面对大量的特征时，想找出一个对训练集拟合很好的模型并不难，但是拟合过度却是更大的问题。</p><h2 id="how-it-works-5"><a class="anchor" href="#how-it-works-5">#</a> How it works...</h2><p>LARS 通过重复选择与残存变化相关的特征。从图上看，相关性实际上就是特征与残差之间的最小角度；这就是 LARS 名称的由来。</p><p>选择第一个特征之后，LARS 会继续沿着最小角的方向移动，直到另一个特征与残差有同样数量的相关性。然后，LARS 会沿着两个特征组合的角度移动。如下图所示：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">def</span> <span class="token function">unit</span><span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="4"></td><td><pre>    squared <span class="token operator">=</span> <span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">,</span> args<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    distance <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>squared<span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token punctuation">(</span><span class="token number">.5</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    <span class="token keyword">return</span> <span class="token builtin">map</span><span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x <span class="token operator">/</span> distance<span class="token punctuation">,</span> args<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>nrows<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylim<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1.1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlim<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1.1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.02</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="15"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> y <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> <span class="token string">r"$x_1$"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre></pre></td></tr><tr><td data-num="17"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="19"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> y <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> <span class="token string">r"$x_2$"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="20"></td><td><pre></pre></td></tr><tr><td data-num="21"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">.45</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="22"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="23"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> y <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> <span class="token string">r"$y$"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="24"></td><td><pre></pre></td></tr><tr><td data-num="25"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"No steps"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="26"></td><td><pre></pre></td></tr><tr><td data-num="27"></td><td><pre><span class="token comment"># step 1</span></pre></td></tr><tr><td data-num="28"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Step 1"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="29"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylim<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1.1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="30"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlim<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1.1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="31"></td><td><pre></pre></td></tr><tr><td data-num="32"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.02</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="33"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="34"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> y <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> <span class="token string">r"$x_1$"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="35"></td><td><pre></pre></td></tr><tr><td data-num="36"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="37"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="38"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> y <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> <span class="token string">r"$x_2$"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="39"></td><td><pre></pre></td></tr><tr><td data-num="40"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="41"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">.5</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> ls<span class="token operator">=</span><span class="token string">'dashed'</span><span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="42"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">.5</span> <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> y <span class="token operator">+</span> <span class="token number">.01</span> <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> <span class="token string">r"$x_2$"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="43"></td><td><pre></pre></td></tr><tr><td data-num="44"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">.47</span><span class="token punctuation">,</span> <span class="token number">.01</span><span class="token punctuation">,</span> width<span class="token operator">=</span><span class="token number">.0015</span><span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="45"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span><span class="token number">.47</span><span class="token operator">-</span><span class="token number">.15</span><span class="token punctuation">,</span> <span class="token number">.01</span> <span class="token operator">+</span> <span class="token number">.03</span><span class="token punctuation">,</span> <span class="token string">"Step 1"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="46"></td><td><pre></pre></td></tr><tr><td data-num="47"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">.45</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="48"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="49"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> y <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> <span class="token string">r"$y$"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="50"></td><td><pre></pre></td></tr><tr><td data-num="51"></td><td><pre><span class="token comment"># step 2</span></pre></td></tr><tr><td data-num="52"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Step 2"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="53"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_ylim<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1.1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="54"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_xlim<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1.1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="55"></td><td><pre></pre></td></tr><tr><td data-num="56"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0.02</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="57"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="58"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> y <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> <span class="token string">r"$x_1$"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="59"></td><td><pre></pre></td></tr><tr><td data-num="60"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="61"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="62"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> y <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> <span class="token string">r"$x_2$"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="63"></td><td><pre></pre></td></tr><tr><td data-num="64"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">.5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="65"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">.5</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> ls<span class="token operator">=</span><span class="token string">'dashed'</span><span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="66"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">.5</span> <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> y <span class="token operator">+</span> <span class="token number">.01</span> <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> <span class="token string">r"$x_2$"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="67"></td><td><pre></pre></td></tr><tr><td data-num="68"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">.47</span><span class="token punctuation">,</span> <span class="token number">.01</span><span class="token punctuation">,</span> width<span class="token operator">=</span><span class="token number">.0015</span><span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="69"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span><span class="token number">.47</span><span class="token operator">-</span><span class="token number">.15</span><span class="token punctuation">,</span> <span class="token number">.01</span> <span class="token operator">+</span> <span class="token number">.03</span><span class="token punctuation">,</span> <span class="token string">"Step 1"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="70"></td><td><pre></pre></td></tr><tr><td data-num="71"></td><td><pre><span class="token comment">##  step 2</span></pre></td></tr><tr><td data-num="72"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">.45</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="73"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">.5</span><span class="token punctuation">,</span> <span class="token number">.02</span><span class="token punctuation">,</span> <span class="token number">.4</span><span class="token punctuation">,</span> <span class="token number">.35</span><span class="token punctuation">,</span> width<span class="token operator">=</span><span class="token number">.0015</span><span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="74"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y <span class="token operator">-</span> <span class="token number">.1</span><span class="token punctuation">,</span> <span class="token string">"Step 2"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="75"></td><td><pre></pre></td></tr><tr><td data-num="76"></td><td><pre>x<span class="token punctuation">,</span> y <span class="token operator">=</span> unit<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">.45</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="77"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>arrow<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> edgecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">,</span> facecolor<span class="token operator">=</span><span class="token string">'black'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="78"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>text<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> y <span class="token operator">+</span> <span class="token number">.05</span><span class="token punctuation">,</span> <span class="token string">r"$y$"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-6-1.png" alt="png"></p><p>具体过程是，我们把<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">x2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord mathnormal">x</span><span class="mord">2</span></span></span></span> 沿着<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">x1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span> 方向移动到一个位置：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">x1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span> 与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span></span></span> 的点积与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">x1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span> 与<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span></span></span> 的点积相同。到了这个位置之后，我们再沿着<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mn>1</mn></mrow><annotation encoding="application/x-tex">x1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord mathnormal">x</span><span class="mord">1</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">x2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord mathnormal">x</span><span class="mord">2</span></span></span></span> 夹角的一半的方向移动。</p><h2 id="theres-more-3"><a class="anchor" href="#theres-more-3">#</a> There's more...</h2><p>和我们前面用交叉检验来优化领回归模型一样，我们可以对 LARS 做交叉检验：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LarsCV</pre></td></tr><tr><td data-num="2"></td><td><pre>lcv <span class="token operator">=</span> LarsCV<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>lcv<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>reg_data<span class="token punctuation">,</span> reg_target<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>d:\Miniconda3\lib\site-packages\sklearn\linear_model\least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 168 iterations, i.e. alpha=2.278e-02, with an active set of 132 regressors, and the smallest cholesky pivot element being 6.144e-08
  ConvergenceWarning)
d:\Miniconda3\lib\site-packages\sklearn\linear_model\least_angle.py:285: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 168 iterations, i.e. alpha=2.105e-02, with an active set of 132 regressors, and the smallest cholesky pivot element being 9.771e-08
  ConvergenceWarning)





LarsCV(copy_X=True, cv=None, eps=2.2204460492503131e-16, fit_intercept=True,
    max_iter=500, max_n_alphas=1000, n_jobs=1, normalize=True,
    precompute='auto', verbose=False)
</code></pre><p>用交叉检验可以帮助我们确定需要使用的非零相关系数的最佳数量。验证如下所示：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>lcv<span class="token punctuation">.</span>coef_ <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>43
</code></pre><p>说实话，LARS 的精髓还没有领会，抽空会把原文译出来，看各种解释不如看原文。</p><p>[1] Efron, Bradley; Hastie, Trevor; Johnstone, Iain and Tibshirani, Robert(2004). &quot;<span class="exturl" data-url="aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS91cmw/c2E9dCZhbXA7cmN0PWomYW1wO3E9JmFtcDtlc3JjPXMmYW1wO3NvdXJjZT13ZWImYW1wO2NkPTEmYW1wO2NhZD1yamEmYW1wO3VhY3Q9OCZhbXA7dmVkPTBDQjRRRmpBQWFoVUtFd2pKM0kzbGpiREhBaFdKbFlnS0hVMmlBcEEmYW1wO3VybD1odHRwJTNBJTJGJTJGd2ViLnN0YW5mb3JkLmVkdSUyRn5oYXN0aWUlMkZQYXBlcnMlMkZMQVJTJTJGTGVhc3RBbmdsZV8yMDAyLnBkZiZhbXA7ZWk9SXRMUlZjbkRFSW1yb2dUTnhJcUFDUSZhbXA7dXNnPUFGUWpDTkZEUDRaanAtY1BuZHpwTmhxXzhXT3dycnVpN2cmYW1wO3NpZzI9ZU54VkctWklqbnNtZTkzek9URlJPdyZhbXA7YnZtPWJ2Ljk5ODA0MjQ3LGQuY0dV">Least Angle Regression</span>&quot;. Annals of Statistics 32(2): pp. 407–499.doi:10.1214/009053604000000067. MR 2060166.</p><h1 id="用线性方法处理分类问题逻辑回归"><a class="anchor" href="#用线性方法处理分类问题逻辑回归">#</a> 用线性方法处理分类问题 —— 逻辑回归</h1><p>实际上 <code>线性模型</code> 也可以用于 <code>分类任务</code> 。方法是把一个线性模型<span class="red">拟合成某个类型的概率分布</span>，然后用一个函数 <code>建立阈值</code> 来确定结果属于哪一类。</p><h2 id="getting-ready-7"><a class="anchor" href="#getting-ready-7">#</a> Getting ready</h2><p>这里用的函数是经典的 <code>逻辑函数</code> 。一个非常简单的函数：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>t</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">f(x)= \frac 1 {1+e^{-t}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.09077em;vertical-align:-.7693300000000001em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.719556em"><span style="top:-2.989em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord">1</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.7693300000000001em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>它的图形如下图所示：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="4"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>rng <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>log_f <span class="token operator">=</span> np<span class="token punctuation">.</span>apply_along_axis<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span><span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> rng<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Logistic Function between [-5, 5]"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>rng<span class="token punctuation">,</span> log_f<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-7-1.png" alt="png"></p><p>让我们用 <code>make_classification</code> 方法创建一个数据集来进行分类：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_classification</pre></td></tr><tr><td data-num="2"></td><td><pre>X<span class="token punctuation">,</span> y <span class="token operator">=</span> make_classification<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> n_features<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h2 id="how-to-do-it-7"><a class="anchor" href="#how-to-do-it-7">#</a> How to do it...</h2><p><code>LogisticRegression</code> 对象和其他线性模型的用法一样：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression</pre></td></tr><tr><td data-num="2"></td><td><pre>lr <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>我们将把前面的数据作为训练集，最后 200 个数据作为测试集。因为这是随机数据集，所以用最后 200 个数据没问题。但是如果处理具有某种结构的数据，就不能这么做了（例如，你的数据集是时间序列数据）：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X_train <span class="token operator">=</span> X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">200</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="2"></td><td><pre>y_train <span class="token operator">=</span> y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">200</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="3"></td><td><pre>X_test <span class="token operator">=</span> X<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">200</span><span class="token punctuation">:</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="4"></td><td><pre>y_test <span class="token operator">=</span> y<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">200</span><span class="token punctuation">:</span><span class="token punctuation">]</span></pre></td></tr></table></figure><p>在本书后面的内容里，我们将详细介绍交叉检验。这里，我们需要的只是用逻辑回归拟合模型。我们会关注训练集的预测结果，就像关注测试集预测结果一样。经常对比两个数据集预测正确率是个好做法。通常，你在训练集获得的结果更好；模型在测试集上预测失败的比例也至关重要：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>lr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>y_train_predictions <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>y_test_predictions <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span></pre></td></tr></table></figure><p>现在我们有了预测值，让我们看看预测的效果。这里，我们只简单看看预测正确的比例；后面，我们会详细的介绍分类模型效果的评估方法。</p><p>计算很简单，就是用预测正确的数量除以总样本数：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">(</span>y_train_predictions <span class="token operator">==</span> y_train<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token operator">/</span> y_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>0.89375000000000004
</code></pre><p>测试集的效果是：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">(</span>y_test_predictions <span class="token operator">==</span> y_test<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token operator">/</span> y_test<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>0.90500000000000003
</code></pre><p>可以看到，测试集的正确率和训练集的结果差不多。但是实际中通常差别很大。</p><p>现在问题变成，怎么把 <code>逻辑函数</code> 转换成 <code>分类方法</code> 。</p><p>首先，线性回归希望找到一个 <code>线性方程</code> 拟合出给定自变量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span></span></span></span> 条件下因变量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.22222em">Y</span></span></span></span> 的期望值，就是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mi>β</mi></mrow><annotation encoding="application/x-tex">E(Y|X)=x \beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05764em">E</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.22222em">Y</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span>。这里<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.22222em">Y</span></span></span></span> 的值是某个类型发生的概率。因此，我们要解决的分类问题就是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mi>β</mi></mrow><annotation encoding="application/x-tex">E(p|X)=x \beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05764em">E</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span>。然后，只要阈值确定，就会有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mi>X</mi><mi>β</mi></mrow><annotation encoding="application/x-tex">Logit(p) = X \beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:.03588em">g</span><span class="mord mathnormal">i</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="mord mathnormal" style="margin-right:.05278em">β</span></span></span></span>。这个理念的扩展形式可以构成许多形式的回归行为，例如，泊松过程（Poisson）。</p><h2 id="theres-more-4"><a class="anchor" href="#theres-more-4">#</a> There's more...</h2><p>下面的内容你以后肯定会遇到。一种情况是一个类型与其他类型的权重不同；例如，一个类型可能权重很大，99%。这种情况在分类工作中经常遇到。经典案例就是信用卡虚假交易检测，大多数交易都不是虚假交易，但是不同类型误判的成本相差很大。</p><p>让我们建立一个分类问题，类型<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span></span></span> 的不平衡权重 95%，我们看看基本的逻辑回归模型如何处理这类问题：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X<span class="token punctuation">,</span> y <span class="token operator">=</span> make_classification<span class="token punctuation">(</span>n_samples<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">,</span> n_features<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> weights<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">.95</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token builtin">sum</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token comment">#检查不平衡的类型</span></pre></td></tr></table></figure><pre><code>0.0562
</code></pre><p>建立训练集和测试集，然后用逻辑回归拟合：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X_train <span class="token operator">=</span> X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">500</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="2"></td><td><pre>y_train <span class="token operator">=</span> y<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">500</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="3"></td><td><pre>X_test <span class="token operator">=</span> X<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">500</span><span class="token punctuation">:</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="4"></td><td><pre>y_test <span class="token operator">=</span> y<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">500</span><span class="token punctuation">:</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>lr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>y_train_predictions <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>y_test_predictions <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span></pre></td></tr></table></figure><p>现在我们在看看模型拟合的情况：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">(</span>y_train_predictions <span class="token operator">==</span> y_train<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token operator">/</span> y_train<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>0.96711111111111114
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">(</span>y_test_predictions <span class="token operator">==</span> y_test<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token operator">/</span> y_test<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>0.96799999999999997
</code></pre><p>结果看着还不错，但这是说如果我们把一个交易预测成正常交易（或者称为类型 0），那么我们有 95% 左右的可能猜对。如果我们想看看模型对类型 1 的预测情况，可能就不是那么好了：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">(</span>y_test<span class="token punctuation">[</span>y_test<span class="token operator">==</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> y_test_predictions<span class="token punctuation">[</span>y_test<span class="token operator">==</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token operator">/</span> y_test<span class="token punctuation">[</span>y_test<span class="token operator">==</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>0.5
</code></pre><p>如果相比正常交易，我们更关心虚假交易；那么这是由商业规则决定的，我们可能会改变预测正确和预测错误的权重。</p><p>通常情况下，虚假交易与正常交易的权重与训练集的类型权重的倒数一致。但是，因为我们更关心虚假交易，所有让我们用多重采样（oversample）方法来表示虚假交易与正常交易的权重。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>lr <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span>class_weight<span class="token operator">=</span><span class="token punctuation">&#123;</span><span class="token number">0</span><span class="token punctuation">:</span> <span class="token number">.15</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span> <span class="token number">.85</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>lr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>LogisticRegression(C=1.0, class_weight=&#123;0: 0.15, 1: 0.85&#125;, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', penalty='l2', random_state=None,
          solver='liblinear', tol=0.0001, verbose=0)
</code></pre><p>让我们再预测一下结果：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>y_train_predictions <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>y_test_predictions <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_test<span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">(</span>y_test<span class="token punctuation">[</span>y_test<span class="token operator">==</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> y_test_predictions<span class="token punctuation">[</span>y_test<span class="token operator">==</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token operator">/</span> y_test<span class="token punctuation">[</span>y_test<span class="token operator">==</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>0.70833333333333337
</code></pre><p>但是，这么做需要付出什么代价？让我们看看：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">(</span>y_test_predictions <span class="token operator">==</span> y_test<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token operator">/</span> y_test<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>0.93999999999999995
</code></pre><p>可以看到，准确率降低了 3%。这样是否合适由你的问题决定。如果与虚假交易相关的评估成本非常高，那么它就能抵消追踪虚假交易付出的成本。</p><h1 id="贝叶斯岭回归"><a class="anchor" href="#贝叶斯岭回归">#</a> 贝叶斯岭回归</h1><p>在<em>用岭回归弥补线性回归的不足</em>主题中，我们介绍了岭回归优化的限制条件。我们还介绍了相关系数的先验概率分布的贝叶斯解释，将很大程度地影响着先验概率分布，先验概率分布通常均值是 0。</p><p>因此，现在我们就来演示如何 scikit-learn 来应用这种解释。</p><h2 id="getting-ready-8"><a class="anchor" href="#getting-ready-8">#</a> Getting ready</h2><p>岭回归和套索回归（lasso regression）用贝叶斯观点来解释，与频率优化观点解释相反。scikit-learn 只实现了贝叶斯岭回归，但是在 * How it works...* 一节，我们将对比两种回归算法。</p><p>首先，我们创建一个回归数据集：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_regression</pre></td></tr><tr><td data-num="2"></td><td><pre>X<span class="token punctuation">,</span> y <span class="token operator">=</span> make_regression<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> n_informative<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> noise<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h2 id="how-to-do-it-8"><a class="anchor" href="#how-to-do-it-8">#</a> How to do it...</h2><p>我们可以把岭回归加载进来拟合模型：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> BayesianRidge</pre></td></tr><tr><td data-num="2"></td><td><pre>br <span class="token operator">=</span> BayesianRidge<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>有两组相关系数，分别是 <code>alpha_1 / alpha_2</code> 和 <code>lambda_1 / lambda_2</code> 。其中， <code>alpha_*</code> 是先验概率分布的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.0037em">α</span></span></span></span> 超参数， <code>lambda_*</code> 是先验概率分布的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">λ</span></span></span></span> 超参数。</p><p>首先，让我们不调整参数直接拟合模型：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>br<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>br<span class="token punctuation">.</span>coef_</pre></td></tr></table></figure><pre><code>array([ -1.39241213,   0.14671513,  -0.08150797,  37.50250891,
         0.21850082,  -0.78482779,  -0.26717555,  -0.71319956,
         0.7926308 ,   5.74658302])
</code></pre><p>现在，我们来调整超参数，注意观察相关系数的变化：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>br_alphas <span class="token operator">=</span> BayesianRidge<span class="token punctuation">(</span>alpha_1<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> lambda_1<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>br_alphas<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>br_alphas<span class="token punctuation">.</span>coef_</pre></td></tr></table></figure><pre><code>array([ -1.38807423,   0.14050794,  -0.08309391,  37.3032803 ,
         0.2254332 ,  -0.77031801,  -0.27005478,  -0.71632657,
         0.78501276,   5.71928608])
</code></pre><h2 id="how-it-works-6"><a class="anchor" href="#how-it-works-6">#</a> How it works...</h2><p>因为是贝叶斯岭回归，我们假设先验概率分布带有误差和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.0037em">α</span></span></span></span> 参数，先验概率分布都服从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Γ</mi></mrow><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord">Γ</span></span></span></span> 分布。</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Γ</mi></mrow><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord">Γ</span></span></span></span> 分布是一种极具灵活性的分布。不同的形状参数和尺度参数的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Γ</mi></mrow><annotation encoding="application/x-tex">\Gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord">Γ</span></span></span></span> 分布形状有差异。<strong>1e-06</strong> 是 scikit-learn 里面 <code>BayesianRidge</code> 形状参数的默认参数值。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> gamma</pre></td></tr><tr><td data-num="5"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre>form <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token punctuation">:</span> <span class="token string">"loc=&#123;&#125;, scale=&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>g <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token number">1e-06</span><span class="token punctuation">,</span> z<span class="token operator">=</span><span class="token number">1e-06</span><span class="token punctuation">:</span> gamma<span class="token punctuation">.</span>pdf<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>g2 <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token number">1e-06</span><span class="token punctuation">,</span> z<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">:</span> gamma<span class="token punctuation">.</span>pdf<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="10"></td><td><pre>g3 <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token number">1e-06</span><span class="token punctuation">,</span> z<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">:</span> gamma<span class="token punctuation">.</span>pdf<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> z<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>rng <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="13"></td><td><pre></pre></td></tr><tr><td data-num="14"></td><td><pre>ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>rng<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span>g<span class="token punctuation">,</span> rng<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token operator">=</span>form<span class="token punctuation">(</span><span class="token number">1e-06</span><span class="token punctuation">,</span> <span class="token number">1e-06</span><span class="token punctuation">)</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="15"></td><td><pre>ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>rng<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span>g2<span class="token punctuation">,</span> rng<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token operator">=</span>form<span class="token punctuation">(</span><span class="token number">1e-06</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'g'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="16"></td><td><pre>ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>rng<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span>g3<span class="token punctuation">,</span> rng<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> label<span class="token operator">=</span>form<span class="token punctuation">(</span><span class="token number">1e-06</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="17"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Different Shapes of the Gamma Distribution"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="18"></td><td><pre>ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-8-1.png" alt="png"></p><p>你会看到，相关系数最终都会收缩到 0，尤其当形状参数特别小的时候。</p><h3 id="theres-more-5"><a class="anchor" href="#theres-more-5">#</a> There's more...</h3><p>就像我前面介绍的，还有一种套索回归的贝叶斯解释。我们把先验概率分布看出是相关系数的函数；它们本身都是随机数。对于套索回归，我们选择一个可以产生 0 的分布，比如双指数分布（Double Exponential Distribution，也叫 Laplace distribution）。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> scipy<span class="token punctuation">.</span>stats <span class="token keyword">import</span> laplace</pre></td></tr><tr><td data-num="2"></td><td><pre>form <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">,</span> y<span class="token punctuation">:</span> <span class="token string">"loc=&#123;&#125;, scale=&#123;&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>g <span class="token operator">=</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> laplace<span class="token punctuation">.</span>pdf<span class="token punctuation">(</span>x<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>rng <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre>ax<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>rng<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span>g<span class="token punctuation">,</span> rng<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Example of Double Exponential Distribution"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-8-2.png" alt="png"></p><p>留意看 x 轴为 0 处的顶点。这将会使套索回归的相关系数为 0。通过调整超参数，还有可能创建出相关系数为 0 的情况，这由问题的具体情况决定。</p><h1 id="用梯度提升回归从误差中学习"><a class="anchor" href="#用梯度提升回归从误差中学习">#</a> 用梯度提升回归从误差中学习</h1><p>梯度提升回归（Gradient boosting regression，GBR）是一种从它的错误中进行学习的技术。它本质上就是集思广益，集成一堆较差的学习算法进行学习。有两点需要注意：</p><ul><li>每个学习算法准备率都不高，但是它们集成起来可以获得很好的准确率。</li><li>这些学习算法依次应用，也就是说每个学习算法都是在前一个学习算法的错误中学习</li></ul><h2 id="getting-ready-9"><a class="anchor" href="#getting-ready-9">#</a> Getting ready</h2><p>我们还是用基本的回归数据来演示 GBR：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_regression</pre></td></tr><tr><td data-num="3"></td><td><pre>X<span class="token punctuation">,</span> y <span class="token operator">=</span> make_regression<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> noise<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h2 id="how-to-do-it-9"><a class="anchor" href="#how-to-do-it-9">#</a> How to do it...</h2><p>GBR 算是一种集成模型因为它是一个集成学习算法。这种称谓的含义是指 GBR 用许多较差的学习算法组成了一个更强大的学习算法：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> GradientBoostingRegressor <span class="token keyword">as</span> GBR</pre></td></tr><tr><td data-num="2"></td><td><pre>gbr <span class="token operator">=</span> GBR<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>gbr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>gbr_preds <span class="token operator">=</span> gbr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span></pre></td></tr></table></figure><p>很明显，这里应该不止一个模型，但是这种模式现在很简明。现在，让我们用基本回归算法来拟合数据当作参照：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegression</pre></td></tr><tr><td data-num="2"></td><td><pre>lr <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>lr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>lr_preds <span class="token operator">=</span> lr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span></pre></td></tr></table></figure><p>有了参照之后，让我们看看 GBR 算法与线性回归算法效果的对比情况。图像生成可以参照第一章正态随机过程的相关主题，首先需要下面的计算：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>gbr_residuals <span class="token operator">=</span> y <span class="token operator">-</span> gbr_preds</pre></td></tr><tr><td data-num="2"></td><td><pre>lr_residuals <span class="token operator">=</span> y <span class="token operator">-</span> lr_preds</pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt</pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>f<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>ax<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>gbr_residuals<span class="token punctuation">,</span>bins<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'GBR Residuals'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">.5</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="4"></td><td><pre>ax<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>lr_residuals<span class="token punctuation">,</span>bins<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'LR Residuals'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">.5</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"GBR Residuals vs LR Residuals"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'best'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-9-1.png" alt="png"></p><p>看起来好像 GBR 拟合的更好，但是并不明显。让我们用 95% 置信区间（Confidence interval,CI）对比一下：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>percentile<span class="token punctuation">(</span>gbr_residuals<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.5</span><span class="token punctuation">,</span> <span class="token number">97.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([-16.73937398,  15.96258406])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>percentile<span class="token punctuation">(</span>lr_residuals<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2.5</span><span class="token punctuation">,</span> <span class="token number">97.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([-19.03378242,  19.45950191])
</code></pre><p>GBR 的置信区间更小，数据更集中，因此其拟合效果更好；我们还可以对 GBR 算法进行一些调整来改善效果。我用下面的例子演示一下，然后在下一节介绍优化方法：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>n_estimators <span class="token operator">=</span> np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1100</span><span class="token punctuation">,</span> <span class="token number">350</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>gbrs <span class="token operator">=</span> <span class="token punctuation">[</span>GBR<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span>n_estimator<span class="token punctuation">)</span> <span class="token keyword">for</span> n_estimator <span class="token keyword">in</span> n_estimators<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="3"></td><td><pre>residuals <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">for</span> i<span class="token punctuation">,</span> gbr <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>gbrs<span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    gbr<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>    residuals<span class="token punctuation">[</span>gbr<span class="token punctuation">.</span>n_estimators<span class="token punctuation">]</span> <span class="token operator">=</span> y <span class="token operator">-</span> gbr<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X<span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>f<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>colors <span class="token operator">=</span> <span class="token punctuation">&#123;</span><span class="token number">800</span><span class="token punctuation">:</span><span class="token string">'r'</span><span class="token punctuation">,</span> <span class="token number">450</span><span class="token punctuation">:</span><span class="token string">'g'</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">:</span><span class="token string">'b'</span><span class="token punctuation">&#125;</span></pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">for</span> k<span class="token punctuation">,</span> v <span class="token keyword">in</span> residuals<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></pre></td></tr><tr><td data-num="5"></td><td><pre>    ax<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>v<span class="token punctuation">,</span>bins<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'n_estimators: %d'</span> <span class="token operator">%</span> k<span class="token punctuation">,</span> color<span class="token operator">=</span>colors<span class="token punctuation">[</span>k<span class="token punctuation">]</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">.5</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Residuals at Various Numbers of Estimators"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'best'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/2-9-2.png" alt="png"></p><p>图像看着有点混乱，但是依然可以看出随着估计器数据的增加，误差在减少。不过，这并不是一成不变的。首先，我们没有交叉检验过，其次，随着估计器数量的增加，训练时间也会变长。现在我们用数据比较小没什么关系，但是如果数据再放大一两倍问题就出来了。</p><h2 id="how-it-works-7"><a class="anchor" href="#how-it-works-7">#</a> How it works...</h2><p>上面例子中 GBR 的第一个参数是 <code>n_estimators</code> ，指 GBR 使用的学习算法的数量。通常，如果你的设备性能更好，可以把 <code>n_estimators</code> 设置的更大，效果也会更好。还有另外几个参数要说明一下。</p><p>你应该在优化其他参数之前先调整 <code>max_depth</code> 参数。因为每个学习算法都是一颗决策树， <code>max_depth</code> 决定了树生成的节点数。选择合适的节点数量可以更好的拟合数据，而更多的节点数可能造成拟合过度。</p><p><code>loss</code> 参数决定损失函数，也直接影响误差。 <code>ls</code> 是默认值，表示最小二乘法（least squares）。还有最小绝对值差值，Huber 损失和分位数损失（quantiles）等等。</p><div class="tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="ic i-tag"></i> 机器学习</a> <a href="/tags/Scikit-learn-Cookbook/" rel="tag"><i class="ic i-tag"></i> Scikit-learn Cookbook</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2021-12-03 21:28:28" itemprop="dateModified" datetime="2021-12-03T21:28:28+08:00">2021-12-03</time> </span><span id="computer-science/machine-learing/Scikit-learn-Cookbook/第2章 处理线性模型/" class="item leancloud_visitors" data-flag-title="第 2 章 处理线性模型" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="//cdn.jsdelivr.net/gh/Hazuki-295/Hazuki-295.github.io@v1.0.2/images/wechatpay.jpg" alt="Hazuki 叶月 微信支付"><p>微信支付</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Hazuki 叶月 <i class="ic i-at"><em>@</em></i></li><li class="link"><strong>本文链接：</strong> <a href="https://hazuki.cn/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC2%E7%AB%A0%20%E5%A4%84%E7%90%86%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" title="第 2 章 处理线性模型">https://hazuki.cn/computer-science/machine-learing/Scikit-learn-Cookbook/第2章 处理线性模型/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC3%E7%AB%A0%20%E4%BD%BF%E7%94%A8%E8%B7%9D%E7%A6%BB%E5%90%91%E9%87%8F%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva3.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gipesng5oej20zk0m87d4.jpg" title="第3章 使用距离向量构建模型"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> Scikit-learn Cookbook</span><h3>第3章 使用距离向量构建模型</h3></a></div><div class="item right"><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC1%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E5%A4%84%E7%90%86/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tva3.sinaimg.cn&#x2F;mw690&#x2F;6833939bly1gicit31ffoj20zk0m8naf.jpg" title="第1章 模型预处理"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> Scikit-learn Cookbook</span><h3>第1章 模型预处理</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%A4%84%E7%90%86%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">第二章 处理线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">简介</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">线性回归模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready"><span class="toc-number">2.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it"><span class="toc-number">2.2.</span> <span class="toc-text">How to do it...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text">评估线性回归模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-2"><span class="toc-number">3.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-2"><span class="toc-number">3.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works"><span class="toc-number">3.3.</span> <span class="toc-text">How it works...</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#theres-more"><span class="toc-number">3.3.1.</span> <span class="toc-text">There&#39;s more...</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%BC%A5%E8%A1%A5%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="toc-number">4.</span> <span class="toc-text">用岭回归弥补线性回归的不足</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-3"><span class="toc-number">4.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-3"><span class="toc-number">4.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-2"><span class="toc-number">4.3.</span> <span class="toc-text">How it works...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%B2%AD%E5%9B%9E%E5%BD%92%E5%8F%82%E6%95%B0"><span class="toc-number">5.</span> <span class="toc-text">优化岭回归参数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-4"><span class="toc-number">5.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-4"><span class="toc-number">5.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-3"><span class="toc-number">5.3.</span> <span class="toc-text">How it works...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-2"><span class="toc-number">5.4.</span> <span class="toc-text">There&#39;s more...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#lasso%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">6.</span> <span class="toc-text">LASSO 正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-5"><span class="toc-number">6.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-5"><span class="toc-number">6.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-4"><span class="toc-number">6.3.</span> <span class="toc-text">How it works...</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#lasso%E4%BA%A4%E5%8F%89%E6%A3%80%E9%AA%8C"><span class="toc-number">6.3.1.</span> <span class="toc-text">LASSO 交叉检验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lasso%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">6.3.2.</span> <span class="toc-text">LASSO 特征选择</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#lars%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">7.</span> <span class="toc-text">LARS 正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-6"><span class="toc-number">7.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-6"><span class="toc-number">7.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-5"><span class="toc-number">7.3.</span> <span class="toc-text">How it works...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-3"><span class="toc-number">7.4.</span> <span class="toc-text">There&#39;s more...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E7%BA%BF%E6%80%A7%E6%96%B9%E6%B3%95%E5%A4%84%E7%90%86%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">8.</span> <span class="toc-text">用线性方法处理分类问题 —— 逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-7"><span class="toc-number">8.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-7"><span class="toc-number">8.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-4"><span class="toc-number">8.3.</span> <span class="toc-text">There&#39;s more...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-number">9.</span> <span class="toc-text">贝叶斯岭回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-8"><span class="toc-number">9.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-8"><span class="toc-number">9.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-6"><span class="toc-number">9.3.</span> <span class="toc-text">How it works...</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#theres-more-5"><span class="toc-number">9.3.1.</span> <span class="toc-text">There&#39;s more...</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%9B%9E%E5%BD%92%E4%BB%8E%E8%AF%AF%E5%B7%AE%E4%B8%AD%E5%AD%A6%E4%B9%A0"><span class="toc-number">10.</span> <span class="toc-text">用梯度提升回归从误差中学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-9"><span class="toc-number">10.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-9"><span class="toc-number">10.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-7"><span class="toc-number">10.3.</span> <span class="toc-text">How it works...</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC1%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E5%A4%84%E7%90%86/" rel="bookmark" title="第1章 模型预处理">第1章 模型预处理</a></li><li class="active"><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC2%E7%AB%A0%20%E5%A4%84%E7%90%86%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" rel="bookmark" title="第2章 处理线性模型">第2章 处理线性模型</a></li><li><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC3%E7%AB%A0%20%E4%BD%BF%E7%94%A8%E8%B7%9D%E7%A6%BB%E5%90%91%E9%87%8F%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B/" rel="bookmark" title="第3章 使用距离向量构建模型">第3章 使用距离向量构建模型</a></li><li><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC4%E7%AB%A0%20%E4%BD%BF%E7%94%A8%20scikit-learn%20%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%88%86%E7%B1%BB/" rel="bookmark" title="第4章 使用 scikit-learn 对数据分类">第4章 使用 scikit-learn 对数据分类</a></li><li><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC5%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E5%90%8E%E5%A4%84%E7%90%86/" rel="bookmark" title="第5章 模型后处理">第5章 模型后处理</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Hazuki 叶月" data-src="//cdn.jsdelivr.net/gh/Hazuki-295/Hazuki-295.github.io@v1.0.2/images/avatar.jpg"><p class="name" itemprop="name">Hazuki 叶月</p><div class="description" itemprop="description">计算机基础 & 编程笔记</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">10</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">7</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">8</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL0hhenVraS0yOTU=" title="https:&#x2F;&#x2F;github.com&#x2F;Hazuki-295"><i class="ic i-github"></i></span> <span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS9sdW8tc2hlbmctNzYtMjItMzg=" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;luo-sheng-76-22-38"><i class="ic i-zhihu"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly95Lm11c2ljLjE2My5jb20vbS91c2VyP2lkPTMwOTUzOTk5NA==" title="https:&#x2F;&#x2F;y.music.163.com&#x2F;m&#x2F;user?id&#x3D;309539994"><i class="ic i-cloud-music"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-magic"></i>链环</a><ul class="submenu"><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC3%E7%AB%A0%20%E4%BD%BF%E7%94%A8%E8%B7%9D%E7%A6%BB%E5%90%91%E9%87%8F%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC1%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E5%A4%84%E7%90%86/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/hexo/" title="分类于 Hexo 博客搭建">Hexo 博客搭建</a> <i class="ic i-angle-right"></i> <a href="/categories/hexo/Theme-Shoka/" title="分类于 Shoka 主题">Shoka 主题</a></div><span><a href="/hexo/Theme.Shoka/01_Shoka/" title="Shoka主题：修改切换标签时更换标题">Shoka主题：修改切换标签时更换标题</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" title="分类于 Scikit-learn Cookbook">Scikit-learn Cookbook</a></div><span><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC2%E7%AB%A0%20%E5%A4%84%E7%90%86%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" title="第2章 处理线性模型">第2章 处理线性模型</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" title="分类于 Scikit-learn Cookbook">Scikit-learn Cookbook</a></div><span><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC1%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E5%A4%84%E7%90%86/" title="第1章 模型预处理">第1章 模型预处理</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" title="分类于 Scikit-learn Cookbook">Scikit-learn Cookbook</a></div><span><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC4%E7%AB%A0%20%E4%BD%BF%E7%94%A8%20scikit-learn%20%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%88%86%E7%B1%BB/" title="第4章 使用 scikit-learn 对数据分类">第4章 使用 scikit-learn 对数据分类</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" title="分类于 Scikit-learn Cookbook">Scikit-learn Cookbook</a></div><span><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC5%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E5%90%8E%E5%A4%84%E7%90%86/" title="第5章 模型后处理">第5章 模型后处理</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/computer-network/" title="分类于 计算机网络">计算机网络</a></div><span><a href="/computer-science/computer-network/transport-layer/" title="第3章 传输层">第3章 传输层</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/operating-system/" title="分类于 操作系统">操作系统</a></div><span><a href="/computer-science/operating-system/Chapter1-Introduction/" title="Chapter 1 Introduction">Chapter 1 Introduction</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/computer-network/" title="分类于 计算机网络">计算机网络</a></div><span><a href="/computer-science/computer-network/network-layer-1/" title="第4章 网络层：数据平面">第4章 网络层：数据平面</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" title="分类于 Scikit-learn Cookbook">Scikit-learn Cookbook</a></div><span><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC3%E7%AB%A0%20%E4%BD%BF%E7%94%A8%E8%B7%9D%E7%A6%BB%E5%90%91%E9%87%8F%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B/" title="第3章 使用距离向量构建模型">第3章 使用距离向量构建模型</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/computer-network/" title="分类于 计算机网络">计算机网络</a></div><span><a href="/computer-science/computer-network/application-layer/" title="第2章 应用层">第2章 应用层</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Hazuki 叶月 @ Hazuki の 小屋</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">160k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">2:26</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"computer-science/machine-learing/Scikit-learn-Cookbook/第2章 处理线性模型/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="//cdn.jsdelivr.net/gh/Hazuki-295/Hazuki-295.github.io@v1.0.2/js/app.js?v=0.2.5"></script></body></html>
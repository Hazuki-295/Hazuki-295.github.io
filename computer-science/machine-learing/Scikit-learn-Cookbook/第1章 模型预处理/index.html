<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" href="https://hazuki.cn/rss.xml"><link rel="alternate" type="application/atom+xml" href="https://hazuki.cn/atom.xml"><link rel="alternate" type="application/json" href="https://hazuki.cn/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="机器学习,Scikit-learn Cookbook"><link rel="canonical" href="https://hazuki.cn/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC1%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E5%A4%84%E7%90%86/"><title>第 1 章 模型预处理 - Scikit-learn Cookbook - 机器学习 - 计算机科学 | Hazuki の 小屋 = = 葉月書架</title><meta name="generator" content="Hexo 5.4.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">第 1 章 模型预处理</h1><div class="meta"><span class="item" title="创建时间：2021-12-03 18:44:50"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2021-12-03T18:44:50+08:00">2021-12-03</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>39k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>36 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Hazuki の 小屋</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://tva3.sinaimg.cn/large/deef1a0cly8gy0yihkpsej21e00u07at.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/deef1a0cly8gy11cqpw4zj21e00u0n6s.jpg"></li><li class="item" data-background-image="https://tva4.sinaimg.cn/large/deef1a0cly8gy122s02qbj20yt0u0gro.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/deef1a0cly8gy0wt2eo0lj21hc0u0qb4.jpg"></li><li class="item" data-background-image="https://tva2.sinaimg.cn/large/deef1a0cly8gy12epa39jj21he0u0wog.jpg"></li><li class="item" data-background-image="http://wx4.sinaimg.cn/large/6833939bly1gipeu1usa7j20zk0m8b29.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/computer-science/" itemprop="item" rel="index" title="分类于 计算机科学"><span itemprop="name">计算机科学</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/computer-science/machine-learing/" itemprop="item" rel="index" title="分类于 机器学习"><span itemprop="name">机器学习</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" itemprop="item" rel="index" title="分类于 Scikit-learn Cookbook"><span itemprop="name">Scikit-learn Cookbook</span></a><meta itemprop="position" content="3"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://hazuki.cn/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC1%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E5%A4%84%E7%90%86/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Hazuki 叶月"><meta itemprop="description" content="葉月書架, 计算机基础 & 编程笔记"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content=""></span><div class="body md" itemprop="articleBody"><div class="note info"><p>本文转载自：<span class="exturl" data-url="aHR0cHM6Ly9naXRlZS5jb20vd2l6YXJkZm9yY2VsL3NrbGVhcm4tY2I=">https://gitee.com/wizardforcel/sklearn-cb</span><br>未经作者的许可，此代码仅用于学习，不能用于其他用途。</p></div><h1 id="第一章-模型预处理"><a class="anchor" href="#第一章-模型预处理">#</a> 第一章 模型预处理</h1><h2 id="简介"><a class="anchor" href="#简介">#</a> 简介</h2><div class="note primary"><p>本章介绍 <code>数据获取</code> （setting data）， <code>数据整理</code> （preparing data）和 <code>建模前的降维</code> （premodel dimensionality reduction）工作。这些内容并非 <code>机器学习</code> （machine learning，ML）最核心的部分，但是它们往往决定模型的成败。</p></div><p>本章主要分三部分。首先，我们介绍如何创建 <code>模拟数据</code> （fake data），这看着微不足道，但是创建模拟数据并用模型进行拟合是模型测试的重要步骤。更重要的是，当我们从零开始一行一行代码实现一个算法时，我们想知道算法功能是否达到预期，这时手上可能没有数据，我们可以创建模拟数据来测试。之后，我们将介绍一些 <code>数据预处理变换</code> 的方法，包括缺失数据填补（data imputation），分类变量编码（categorical variable encoding）。最后，我们介绍一些 <code>降维方法</code> ，如主成分分析，因子分析，以及正态随机过程等。</p><p>本章，尤其是前半部分与后面的章节衔接紧密。后面使用 <code>scikit-learn</code> 时，数据都源自本章内容。前两节介绍数据获取；紧接着介绍数据清洗。</p><div class="note info no-icon"><p>本书使用 scikit-learn 0.15，NumPy 1.9 和 pandas 0.13，兼容 Python2.7 和 Python3.4。还会用到其他的 Python 库，建议参考对应的官方安装指令。</p></div><h1 id="从外部源获取样本数据"><a class="anchor" href="#从外部源获取样本数据">#</a> 从外部源获取样本数据</h1><p>如果条件允许，学本书内容时尽量用你熟悉的数据集；方便起见，我们用 <code>scikit-learn</code> 的<span class="red">内置数据库</span>。这些内置数据库可用于测试不同的建模技术，如回归和分类。而且这些内置数据库都是非常著名的数据库。这对不同领域的学术论文的作者们来说是很用的，他们可以用这些内置数据库将他们的模型与其他模型进行比较。</p><div class="note success"><p>推荐使用 IPython 来运行文中的指令。大内存很重要，这样可以让普通的命令正常运行。如果用 IPython Notebook 就更好了。如果你用 Notebook，记得用 <code>%matplotlib inline</code> 指令，这样图象就会出现在 Notebook 里面，而不是一个新窗口里。</p></div><h2 id="getting-ready"><a class="anchor" href="#getting-ready">#</a> Getting ready</h2><p>scikit-learn 的内置数据库在 <code>datasets</code> 模块里。用如下命令导入：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr></table></figure><p>在 IPython 里面运行 <code>datasets.*?</code> 就会看到 <code>datasets</code> 模块的指令列表。</p><h2 id="how-to-do-it"><a class="anchor" href="#how-to-do-it">#</a> How to do it…</h2><p><code>datasets</code> 模块主要有两种数据类型。较小的测试数据集在 <code>sklearn</code> 包里面，可以通过 <code>datasets.load_*?</code> 查看。较大的数据集可以根据需要下载。后者默认情况下不在 <code>sklearn</code> 包里面；但是，有时这些大数据集可以更好的测试模型和算法，因为比较复杂足以模拟现实情形。</p><p>默认在 <code>sklearn</code> 包里面的数据集可以通过 <code>datasets.load_*?</code> 查看。另外一些数据集需要通过 <code>datasets.fetch_*?</code> 下载，这些数据集更大，没有被自动安装。经常用于测试那些解决实际问题的算法。</p><p>首先，加载 <code>boston</code> 数据集看看：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>boston <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>boston<span class="token punctuation">.</span>DESCR<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>Boston House Prices dataset

Notes
------
Data Set Characteristics:  

    :Number of Instances: 506 

    :Number of Attributes: 13 numeric/categorical predictive
    
    :Median Value (attribute 14) is usually the target

    :Attribute Information (in order):
        - CRIM     per capita crime rate by town
        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
        - INDUS    proportion of non-retail business acres per town
        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
        - NOX      nitric oxides concentration (parts per 10 million)
        - RM       average number of rooms per dwelling
        - AGE      proportion of owner-occupied units built prior to 1940
        - DIS      weighted distances to five Boston employment centres
        - RAD      index of accessibility to radial highways
        - TAX      full-value property-tax rate per $10,000
        - PTRATIO  pupil-teacher ratio by town
        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
        - LSTAT    % lower status of the population
        - MEDV     Median value of owner-occupied homes in $1000's

    :Missing Attribute Values: None

    :Creator: Harrison, D. and Rubinfeld, D.L.

This is a copy of UCI ML housing dataset.
http://archive.ics.uci.edu/ml/datasets/Housing


This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.

The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic
prices and the demand for clean air', J. Environ. Economics &amp; Management,
vol.5, 81-102, 1978.   Used in Belsley, Kuh &amp; Welsch, 'Regression diagnostics
...', Wiley, 1980.   N.B. Various transformations are used in the table on
pages 244-261 of the latter.

The Boston house-price data has been used in many machine learning papers that address regression
problems.   
     
**References**

   - Belsley, Kuh &amp; Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.
   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.
   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)
</code></pre><p><code>DESCR</code> 将列出数据集的一些概况。下面我们来下载一个数据集：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>housing <span class="token operator">=</span> datasets<span class="token punctuation">.</span>fetch_california_housing<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span>housing<span class="token punctuation">.</span>DESCR<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>downloading Cal. housing from http://lib.stat.cmu.edu/modules.php?op=modload&amp;name=Downloads&amp;file=index&amp;req=getit&amp;lid=83 to C:\Users\tj2\scikit_learn_data
California housing dataset.

The original database is available from StatLib

    http://lib.stat.cmu.edu/

The data contains 20,640 observations on 9 variables.

This dataset contains the average house value as target variable
and the following input variables (features): average income,
housing average age, average rooms, average bedrooms, population,
average occupation, latitude, and longitude in that order.

References
----------

Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
Statistics and Probability Letters, 33 (1997) 291-297.
</code></pre><h2 id="how-it-works"><a class="anchor" href="#how-it-works">#</a> How it works…</h2><div class="note primary"><p>当这些数据集被加载时，它们并不是直接转换成 Numpy 数组。它们是 <code>Bunch</code> 类型。<strong>Bunch</strong> 是 Python 常用的数据结构。基本可以看成是一个词典，它的键被实例对象作为 <code>属性</code> 使用。</p></div><p>用 <code>data</code> 属性连接数据中包含 <code>自变量</code> 的 Numpy 数组，用 <code>target</code> 属性连接数据中的 <code>因变量</code> 。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X<span class="token punctuation">,</span> y <span class="token operator">=</span> boston<span class="token punctuation">.</span>data<span class="token punctuation">,</span> boston<span class="token punctuation">.</span>target</pre></td></tr></table></figure><p>网络上 <code>Bunch</code> 对象有不同的实现；自己写一个也不难。scikit-learn 用<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NjaWtpdC1sZWFybi9zY2lraXQtbGVhcm4vYmxvYi9tYXN0ZXIvc2tsZWFybi9kYXRhc2V0cy9iYXNlLnB5">基本模块</span>定义 <code>Bunch</code> 。</p><h2 id="theres-more"><a class="anchor" href="#theres-more">#</a> There's more…</h2><p>让你从外部源获取数据集时，它默认会被当前文件夹的 <code>scikit_learn_data/</code> 放在里面，可以通过两种方式进行配置：</p><ul><li>设置 <code>SCIKIT_LEARN_DATA</code> 环境变量指定下载位置</li><li><code>fetch_*?</code> 方法的第一个参数是 <code>data_home</code> ，可以知道下载位置</li></ul><p>通过 <code>datasets.get_data_home()</code> 很容易检查默认下载位置。</p><h2 id="see-also"><a class="anchor" href="#see-also">#</a> See also</h2><p>UCI 机器学习库（UCI Machine Learning Repository）是找简单数据集的好地方。很多 scikit-learn 的数据集都在那里，那里还有更多的数据集。其他数据源还是著名的 KDD 和 Kaggle。</p><h1 id="创建试验样本数据"><a class="anchor" href="#创建试验样本数据">#</a> 创建试验样本数据</h1><p>希望你在学习本书时用自己的数据来试验，如果实在没有数据，下面就介绍如何用 scikit-learn 创建一些试验用的 <code>样本数据</code> （toy data）。</p><h2 id="getting-ready-2"><a class="anchor" href="#getting-ready-2">#</a> Getting ready</h2><p>与前面获取内置数据集，获取新数据集的过程类似，创建样本数据集，用 <code>make_数据集名称</code> 函数。这些数据集都是人造的：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre></pre></td></tr><tr><td data-num="3"></td><td><pre>datasets<span class="token punctuation">.</span>make_<span class="token operator">*</span>?</pre></td></tr></table></figure><pre><code>datasets.make_biclusters
datasets.make_blobs
datasets.make_checkerboard
datasets.make_circles
datasets.make_classification
datasets.make_friedman1
datasets.make_friedman2
datasets.make_friedman3
datasets.make_gaussian_quantiles
datasets.make_hastie_10_2
datasets.make_low_rank_matrix
datasets.make_moons
datasets.make_multilabel_classification
datasets.make_regression
datasets.make_s_curve
datasets.make_sparse_coded_signal
datasets.make_sparse_spd_matrix
datasets.make_sparse_uncorrelated
datasets.make_spd_matrix
datasets.make_swiss_roll
</code></pre><p>为了简便，下面我们用 <code>d</code> 表示 <code>datasets</code> ， <code>np</code> 表示 <code>numpy</code> ：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">as</span> d</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr></table></figure><h2 id="how-to-do-it-2"><a class="anchor" href="#how-to-do-it-2">#</a> How to do it...</h2><p>这一节将带你创建几个数据集；在后面的 <em>How it works...</em> 一节，我们会检验这些数据集的特性。除了样本数据集，后面还会创建一些具有特定属性的数据集来显示算法的特点。</p><h3 id="回归数据集"><a class="anchor" href="#回归数据集">#</a> 回归数据集</h3><p>首先，我们创建 <code>回归（regression）数据集</code> ：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>reg_data <span class="token operator">=</span> d<span class="token punctuation">.</span>make_regression<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>reg_data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">,</span>reg_data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape</pre></td></tr></table></figure><pre><code>((100, 100), (100,))
</code></pre><p><code>reg_data</code> 默认是一个元组，第一个元素是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>100</mn><mo>×</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">100\times100</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span></span></span></span> 的矩阵 ——100 个样本，每个样本 10 个特征（自变量），第二个元素是 1 个因变量，对应自变量的样本数量，也是 100 个样本。然而，默认情况下，只有 10 个特征与因变量的相关（参数 <code>n_informative</code> 默认值是 10），其他 90 个特征都与因变量无关。</p><p>可以自定义更复杂的数据集。比如，创建一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1000</mn><mo>×</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">1000\times10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span><span class="mord">0</span></span></span></span> 的矩阵，5 个特征与因变量相关，误差系数 0.2，两个因变量。代码如下所示：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>complex_reg_data <span class="token operator">=</span> d<span class="token punctuation">.</span>make_regression<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>complex_reg_data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">,</span>complex_reg_data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape</pre></td></tr></table></figure><pre><code>((1000, 10), (1000, 2))
</code></pre><h3 id="分类数据集"><a class="anchor" href="#分类数据集">#</a> 分类数据集</h3><p><code>分类数据集</code> 也很容易创建。很容易创建一个 <code>基本均衡</code> 分类集，但是这种情况现实中几乎不可能发生 —— 大多数用户不会改变消费习惯，大多数交易都不是虚假的，等等。因此，创建一个 <code>非均衡</code> 数据集更有意义：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>classification_set <span class="token operator">=</span> d<span class="token punctuation">.</span>make_classification<span class="token punctuation">(</span>weights<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>np<span class="token punctuation">.</span>bincount<span class="token punctuation">(</span>classification_set<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([10, 90], dtype=int64)
</code></pre><h3 id="聚类数据集"><a class="anchor" href="#聚类数据集">#</a> 聚类数据集</h3><p><code>聚类数据集</code> 也可以创建。有一些函数可以为不同聚类算法创建对应的数据集。例如， <code>blobs</code> 函数可以轻松创建 K-Means 聚类数据集：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">as</span> d</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="4"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>blobs <span class="token operator">=</span> d<span class="token punctuation">.</span>make_blobs<span class="token punctuation">(</span><span class="token number">200</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre></pre></td></tr><tr><td data-num="8"></td><td><pre>f <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr><tr><td data-num="10"></td><td><pre>ax <span class="token operator">=</span> f<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="11"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"A blob with 3 centers"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre></pre></td></tr><tr><td data-num="13"></td><td><pre>colors <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'r'</span><span class="token punctuation">,</span> <span class="token string">'g'</span><span class="token punctuation">,</span> <span class="token string">'b'</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="14"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>blobs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> blobs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> color<span class="token operator">=</span>colors<span class="token punctuation">[</span>blobs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.75</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>&lt;matplotlib.collections.PathCollection at 0x88e44e0&gt;
</code></pre><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-2-1.png" alt="png"></p><h2 id="how-it-works-2"><a class="anchor" href="#how-it-works-2">#</a> How it works...</h2><p>下面让我们从源代码看看 scikit-learn 是如何生成回归数据集的。下面任何未重新定义的参数都使用 <code>make_regression</code> 函数的默认值。</p><p>其实非常简单。首先，函数调用时生成一个指定维度的随机数组。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>n_samples<span class="token punctuation">,</span> n_features<span class="token punctuation">)</span></pre></td></tr></table></figure><p>对于基本均衡数据集，其目标数据集生成方法是：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>ground_truth <span class="token operator">=</span> np<span class="token punctuation">.</span>zeroes<span class="token punctuation">(</span><span class="token punctuation">(</span>np_samples<span class="token punctuation">,</span> n_target<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>ground_truth<span class="token punctuation">[</span><span class="token punctuation">:</span>n_informative<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">100</span><span class="token operator">*</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>n_informative<span class="token punctuation">,</span> n_targets<span class="token punctuation">)</span></pre></td></tr></table></figure><p>然后 <code>X</code> 和 <code>ground_truth</code> 点积加上 <code>bias</code> 就得到了 <code>y</code> ：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>y <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X<span class="token punctuation">,</span> ground_truth<span class="token punctuation">)</span> <span class="token operator">+</span> bias</pre></td></tr></table></figure><div class="note success"><p>点积是一种基本的矩阵运算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>A</mi><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow></msub><mo>⋅</mo><msub><mi>B</mi><mrow><mi>n</mi><mo>×</mo><mi>s</mi></mrow></msub><mo>=</mo><msub><mi>C</mi><mrow><mi>m</mi><mo>×</mo><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">A_{m \times n} \cdot B_{n \times s} = C_{m \times s}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.891661em;vertical-align:-.208331em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.25833100000000003em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.208331em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.891661em;vertical-align:-.208331em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05017em">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.25833100000000003em"><span style="top:-2.5500000000000003em;margin-left:-.05017em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.208331em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.891661em;vertical-align:-.208331em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.25833100000000003em"><span style="top:-2.5500000000000003em;margin-left:-.07153em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.208331em"><span></span></span></span></span></span></span></span></span></span>。因此， <code>y</code> 数据集里面样本数量是 <code>n_samples</code> ，即数据集的行数，因变量数量是 <code>n_target</code> 。</p></div><p>由于 Numpy 的 <code>广播操作（broadcasting）</code> ， <code>bias</code> 虽然是标量，也会被增加到矩阵的每个元素上。增加噪声和数据混洗都很简单。这样试验用的回归数据集就完美了。</p><h1 id="把数据调整为标准正态分布"><a class="anchor" href="#把数据调整为标准正态分布">#</a> 把数据调整为标准正态分布</h1><div class="note primary"><p>经常需要将数据 <code>标准化（scaling）</code> 调整为 <code>标准正态分布（standard normal）</code> 。标准正态分布算得上是统计学中最重要的分布了。如果你学过统计，Z 值表（z-scores）应该不陌生。实际上，Z 值表的作用就是把服从某种分布的特征转换成标准正态分布的 Z 值。</p></div><h2 id="getting-ready-3"><a class="anchor" href="#getting-ready-3">#</a> Getting ready</h2><p>数据标准化调整是非常有用的。许多机器学习算法在具有不同范围特征的数据中呈现不同的学习效果。例如， <code>SVM</code> （Support Vector Machine，支持向量机）在没有标准化调整过的数据中表现很差，因为可能一个变量的范围是 0-10000，而另一个变量的范围是 0-1。 <code>preprocessing</code> 模块提供了一些函数可以将特征调整为标准形：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> preprocessing</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr></table></figure><h2 id="how-to-do-it-3"><a class="anchor" href="#how-to-do-it-3">#</a> How to do it...</h2><p>还用 <code>boston</code> 数据集运行下面的代码：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre>boston <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>X<span class="token punctuation">,</span> y <span class="token operator">=</span> boston<span class="token punctuation">.</span>data<span class="token punctuation">,</span> boston<span class="token punctuation">.</span>target</pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment">#前三个特征的均值</span></pre></td></tr></table></figure><pre><code>array([  3.59376071,  11.36363636,  11.13677866])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>std<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment">#前三个特征的标准差</span></pre></td></tr></table></figure><pre><code>array([  8.58828355,  23.29939569,   6.85357058])
</code></pre><p>这里看出很多信息。首先，第一个特征的均值是三个特征中最小的，而其标准差却比第三个特征的标准差大。第二个特征的均值和标准差都是最大的 —— 说明它的值很分散，我们通过 <code>preprocessing</code> 对它们标准化：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X_2 <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>scale<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X_2<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([  6.34099712e-17,  -6.34319123e-16,  -2.68291099e-15])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X_2<span class="token punctuation">.</span>std<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([ 1.,  1.,  1.])
</code></pre><h2 id="how-it-works-3"><a class="anchor" href="#how-it-works-3">#</a> How it works...</h2><p><code>中心化与标准化</code> 函数很简单，就是减去均值后除以标准差，公式如下所示：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>x</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mover accent="true"><mi>x</mi><mo>ˉ</mo></mover></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex">x= \frac {x- \bar x} \sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.9463300000000001em;vertical-align:-.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603300000000002em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal" style="margin-right:.03588em">σ</span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.56778em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord mathnormal">x</span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.22222em"><span class="mord">ˉ</span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>除了这个函数，还有一个 <code>中心化与标准化类</code> ，与管线命令（Pipeline）联合处理大数据集时很有用。单独使用一个中心化与标准化类实例也是有用处的：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>my_scaler <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>my_scaler<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>my_scaler<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([  6.34099712e-17,  -6.34319123e-16,  -2.68291099e-15])
</code></pre><p>把特征的样本均值变成 <code>0</code> ，标准差变成 <code>1</code> ，这种标准化处理并不是唯一的方法。 <code>preprocessing</code> 还有 <code>MinMaxScaler</code> 类，将样本数据根据最大值和最小值调整到一个区间内：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>my_minmax_scaler <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>MinMaxScaler<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>my_minmax_scaler<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>my_minmax_scaler<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([ 1.,  1.,  1.])
</code></pre><p>通过 <code>MinMaxScaler</code> 类可以很容易将默认的区间 <code>0</code> 到 <code>1</code> 修改为需要的区间：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>my_odd_scaler <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>MinMaxScaler<span class="token punctuation">(</span>feature_range<span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3.14</span><span class="token punctuation">,</span> <span class="token number">3.14</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>my_odd_scaler<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>my_odd_scaler<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([ 3.14,  3.14,  3.14])
</code></pre><p>还有一种方法是 <code>正态化（normalization）</code> 。它会将每个样本 <code>长度</code> 标准化为 1。这种方法和前面介绍的不同，它的特征值是标量。正态化代码如下：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>normalized_X <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>乍看好像没什么用，但是在求欧式距离（相似度度量指标）时就很必要了。例如三个样本分别是向量 <code>(1,1,0)</code> ， <code>(3,3,0)</code> ， <code>(1,-1,0)</code> 。样本 1 与样本 3 的距离比样本 1 与样本 2 的距离短，尽管样本 1 与样本 3 是轴对称，而样本 1 与样本 2 只是比例不同而已。由于距离常用于相似度检测，因此建模之前如果不对数据进行正态化很可能会造成失误。</p><h2 id="theres-more-2"><a class="anchor" href="#theres-more-2">#</a> There's more...</h2><p>数据填补（data imputation）是一个内涵丰富的主题，在使用 scikit-learn 的数据填补功能时需要注意以下两点。</p><h3 id="创建幂等标准化idempotent-scaler对象"><a class="anchor" href="#创建幂等标准化idempotent-scaler对象">#</a> 创建幂等标准化（idempotent scaler）对象</h3><p>有时可能需要标准化 <code>StandardScaler</code> 实例的均值和 / 或方差。例如，可能（尽管没用）会经过一系列变化创建出一个与原来完全相同的 <code>StandardScaler</code> ：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>my_useless_scaler <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>StandardScaler<span class="token punctuation">(</span>with_mean<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> with_std<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>transformed_sd <span class="token operator">=</span> my_useless_scaler<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>std<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>original_sd <span class="token operator">=</span> X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>std<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>np<span class="token punctuation">.</span>array_equal<span class="token punctuation">(</span>transformed_sd<span class="token punctuation">,</span> original_sd<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>True
</code></pre><h3 id="处理稀疏数据填补"><a class="anchor" href="#处理稀疏数据填补">#</a> 处理稀疏数据填补</h3><p>在标准化处理时，稀疏矩阵的处理方式与正常矩阵没太大不同。这是因为数据经过中心化处理后，原来的 <code>0</code> 值会变成非 <code>0</code> 值，这样稀疏矩阵经过处理就不再稀疏了：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> scipy</pre></td></tr><tr><td data-num="2"></td><td><pre>matrix <span class="token operator">=</span> scipy<span class="token punctuation">.</span>sparse<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>preprocessing<span class="token punctuation">.</span>scale<span class="token punctuation">(</span>matrix<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

&lt;ipython-input-45-466df6030461&gt; in &lt;module&gt;()
      1 import scipy
      2 matrix = scipy.sparse.eye(1000)
----&gt; 3 preprocessing.scale(matrix)


d:\programfiles\Miniconda3\lib\site-packages\sklearn\preprocessing\data.py in scale(X, axis, with_mean, with_std, copy)
    120         if with_mean:
    121             raise ValueError(
--&gt; 122                 &quot;Cannot center sparse matrices: pass `with_mean=False` instead&quot;
    123                 &quot; See docstring for motivation and alternatives.&quot;)
    124         if axis != 0:


ValueError: Cannot center sparse matrices: pass `with_mean=False` instead See docstring for motivation and alternatives.
</code></pre><p>这个错误表面，标准化一个稀疏矩阵不能带 <code>with_mean</code> ，只要 <code>with_std</code> ：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>preprocessing<span class="token punctuation">.</span>scale<span class="token punctuation">(</span>matrix<span class="token punctuation">,</span> with_mean<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>&lt;1000x1000 sparse matrix of type '&lt;class 'numpy.float64'&gt;'
	with 1000 stored elements in Compressed Sparse Row format&gt;
</code></pre><p>另一个方法是直接处理 <code>matrix.todense()</code> 。但是，这个方法很危险，因为矩阵已经是稀疏的了，这么做可能出现内存异常。</p><h1 id="用阈值创建二元特征"><a class="anchor" href="#用阈值创建二元特征">#</a> 用阈值创建二元特征</h1><p>在前一个主题，我们介绍了数据转换成标准正态分布的方法。现在，我们看看另一种完全不同的转换方法。</p><p>当不需要呈标准化分布的数据时，我们可以不处理它们直接使用；但是，如果有足够理由，直接使用也许是聪明的做法。通常，尤其是处理连续数据时，可以通过建立二元特征来分割数据。</p><h2 id="getting-ready-4"><a class="anchor" href="#getting-ready-4">#</a> Getting ready</h2><p>通常建立二元特征是非常有用的方法，不过要格外小心。我们还是用 <code>boston</code> 数据集来学习如何创建二元特征。</p><p>首先，加载 <code>boston</code> 数据集：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre>boston <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr></table></figure><h2 id="how-to-do-it-4"><a class="anchor" href="#how-to-do-it-4">#</a> How to do it...</h2><p>与标准化处理类似，scikit-learn 有两种方法二元特征：</p><ul><li><code>preprocessing.binarize</code> （一个函数）</li><li><code>preprocessing.Binarizer</code> （一个类）</li></ul><p><code>boston</code> 数据集的因变量是房子的价格中位数（单位：千美元）。这个数据集适合测试回归和其他连续型预测算法，但是假如现在我们想预测一座房子的价格是否高于总体均值。要解决这个问题，我们需要创建一个均值的阈值。如果一个值比均值大，则为 <code>1</code> ；否则，则为 <code>0</code> ：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> preprocessing</pre></td></tr><tr><td data-num="2"></td><td><pre>new_target <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>binarize<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>target<span class="token punctuation">,</span> threshold<span class="token operator">=</span>boston<span class="token punctuation">.</span>target<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>new_target<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([ 1.,  0.,  1.,  1.,  1.])
</code></pre><p>很容易，让我们检查一下：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token punctuation">(</span>boston<span class="token punctuation">.</span>target<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span> <span class="token operator">></span> boston<span class="token punctuation">.</span>target<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([1, 0, 1, 1, 1])
</code></pre><p>既然 Numpy 已经很简单了，为什么还要用 scikit-learn 的函数呢？管道命令，将在<em>用管道命令联接多个预处理步骤</em>一节中介绍，会解释这个问题；要用管道命令就要用 <code>Binarizer</code> 类：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token builtin">bin</span> <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>Binarizer<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>target<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>new_target <span class="token operator">=</span> <span class="token builtin">bin</span><span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>boston<span class="token punctuation">.</span>target<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>new_target<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([ 1.,  0.,  1.,  1.,  1.])
</code></pre><h2 id="how-it-works-4"><a class="anchor" href="#how-it-works-4">#</a> How it works...</h2><p>方法看着非常简单；其实 scikit-learn 在底层创建一个检测层，如果被监测的值比阈值大就返回 <code>Ture</code> 。然后把满足条件的值更新为 <code>1</code> ，不满足条件的更新为 <code>0</code> 。</p><h2 id="theres-more-3"><a class="anchor" href="#theres-more-3">#</a> There's more...</h2><p>让我们再介绍一些稀疏矩阵和 <code>fit</code> 方法的知识。</p><h3 id="稀疏矩阵"><a class="anchor" href="#稀疏矩阵">#</a> 稀疏矩阵</h3><p>稀疏矩阵的 <code>0</code> 是不被存储的；这样可以节省很多空间。这就为 <code>binarizer</code> 造成了问题，需要指定阈值参数 <code>threshold</code> 不小于 <code>0</code> 来解决，如果 <code>threshold</code> 小于 <code>0</code> 就会出现错误：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> scipy<span class="token punctuation">.</span>sparse <span class="token keyword">import</span> coo</pre></td></tr><tr><td data-num="2"></td><td><pre>spar <span class="token operator">=</span> coo<span class="token punctuation">.</span>coo_matrix<span class="token punctuation">(</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>binomial<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">.25</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>preprocessing<span class="token punctuation">.</span>binarize<span class="token punctuation">(</span>spar<span class="token punctuation">,</span> threshold<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

&lt;ipython-input-31-c9b5156c63ab&gt; in &lt;module&gt;()
      1 from scipy.sparse import coo
      2 spar = coo.coo_matrix(np.random.binomial(1, .25, 100))
----&gt; 3 preprocessing.binarize(spar, threshold=-1)


d:\programfiles\Miniconda3\lib\site-packages\sklearn\preprocessing\data.py in binarize(X, threshold, copy)
    718     if sparse.issparse(X):
    719         if threshold &lt; 0:
--&gt; 720             raise ValueError('Cannot binarize a sparse matrix with threshold '
    721                              '&lt; 0')
    722         cond = X.data &gt; threshold


ValueError: Cannot binarize a sparse matrix with threshold &lt; 0
</code></pre><h3 id="fit方法"><a class="anchor" href="#fit方法">#</a> <code>fit</code> 方法</h3><p><code>binarizer</code> 类里面有 <code>fit</code> 方法，但是它只是通用接口，并没有实际的拟合操作，仅返回对象。</p><h1 id="分类变量处理"><a class="anchor" href="#分类变量处理">#</a> 分类变量处理</h1><p>分类变量是经常遇到的问题。一方面它们提供了信息；另一方面，它们可能是文本形式 —— 纯文字或者与文字相关的整数 —— 就像表格的索引一样。</p><p>因此，我们在建模的时候往往需要将这些变量量化，但是仅仅用简单的 <code>id</code> 或者原来的形式是不行的。因为我们也需要避免在上一节里<em>通过阈值创建二元特征</em>遇到的问题。如果我们把数据看成是连续的，那么也必须解释成连续的。</p><h2 id="getting-ready-5"><a class="anchor" href="#getting-ready-5">#</a> Getting ready</h2><p>这里 <code>boston</code> 数据集不适合演示。虽然它适合演示二元特征，但是用来创建分类变量不太合适。因此，这里用 <code>iris</code> 数据集演示。</p><p>解决问题之前先把问题描述清楚。假设有一个问题，其目标是预测花萼的宽度；那么花的种类就可能是一个有用的特征。</p><p>首先，让我们导入数据：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre>iris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>X <span class="token operator">=</span> iris<span class="token punctuation">.</span>data</pre></td></tr><tr><td data-num="4"></td><td><pre>y <span class="token operator">=</span> iris<span class="token punctuation">.</span>target</pre></td></tr></table></figure><p>现在 <code>X</code> 和 <code>y</code> 都获得了对应的值，我们把它们放到一起：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre>d <span class="token operator">=</span> np<span class="token punctuation">.</span>column_stack<span class="token punctuation">(</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h2 id="how-to-do-it-5"><a class="anchor" href="#how-to-do-it-5">#</a> How to do it...</h2><p>下面我们把花类型 <code>y</code> 对应那一列转换成分类特征：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> preprocessing</pre></td></tr><tr><td data-num="2"></td><td><pre>text_encoder <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>OneHotEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>text_encoder<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>d<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 1.,  0.,  0.],
       [ 1.,  0.,  0.],
       [ 1.,  0.,  0.],
       [ 1.,  0.,  0.],
       [ 1.,  0.,  0.]])
</code></pre><h2 id="how-it-works-5"><a class="anchor" href="#how-it-works-5">#</a> How it works...</h2><p>这里，编码器为每个分类变量创建了额外的特征，转变成一个稀疏矩阵。矩阵是这样定义的：每一行由 0 和 1 构成，对应的分类特征是 1，其他都是 0。用稀疏矩阵存储数据很合理。</p><p><code>text_encoder</code> 是一个标准的 scikit-learn 模型，可以重复使用：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>text_encoder<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([[ 0.,  1.,  0.],
       [ 0.,  1.,  0.],
       [ 0.,  1.,  0.]])
</code></pre><h2 id="theres-more-4"><a class="anchor" href="#theres-more-4">#</a> There's more...</h2><p>在 scikit-learn 和 Python 库中，还有一些方法可以创建分类变量。如果你更倾向于用 scikit-learn，而且分类编码原则很简单，可以试试 <code>DictVectorizer</code> 。如果你需要处理更复杂的分类编码原则， <code>patsy</code> 是很好的选择。</p><h3 id="dictvectorizer"><a class="anchor" href="#dictvectorizer">#</a> DictVectorizer</h3><p><code>DictVectorizer</code> 可以将字符串转换成分类特征：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction <span class="token keyword">import</span> DictVectorizer</pre></td></tr><tr><td data-num="2"></td><td><pre>dv <span class="token operator">=</span> DictVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>my_dict <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">&#123;</span><span class="token string">'species'</span><span class="token punctuation">:</span> iris<span class="token punctuation">.</span>target_names<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">&#125;</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> y<span class="token punctuation">]</span></pre></td></tr><tr><td data-num="4"></td><td><pre>dv<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>my_dict<span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 1.,  0.,  0.],
       [ 1.,  0.,  0.],
       [ 1.,  0.,  0.],
       [ 1.,  0.,  0.],
       [ 1.,  0.,  0.]])
</code></pre><blockquote><p>Python 的词典可以看成是一个稀疏矩阵，它们只包含非 0 值。</p></blockquote><h3 id="pasty"><a class="anchor" href="#pasty">#</a> Pasty</h3><p><code>patsy</code> 是另一个分类变量编码的包。经常和 <code>StatsModels</code> 一起用， <code>patsy</code> 可以把一组字符串转换成一个矩阵。</p><blockquote><p>这部分内容与 scikit-learn 关系不大，跳过去也没关系。</p></blockquote><p>例如，如果 <code>x</code> 和 <code>y</code> 都是字符串， <code>dm = patsy.design_matrix(&quot;x + y&quot;)</code> 将创建适当的列。如果不是， <code>C(x)</code> 将生成一个分类变量。</p><p>例如，初看 <code>iris.target</code> ，可以把它当做是一个连续变量。因此，用下面的命令处理：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> patsy</pre></td></tr><tr><td data-num="2"></td><td><pre>patsy<span class="token punctuation">.</span>dmatrix<span class="token punctuation">(</span><span class="token string">"0 + C(species)"</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span><span class="token string">'species'</span><span class="token punctuation">:</span> iris<span class="token punctuation">.</span>target<span class="token punctuation">&#125;</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>DesignMatrix with shape (150, 3)
  C(species)[0]  C(species)[1]  C(species)[2]
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
              1              0              0
  [120 rows omitted]
  Terms:
    'C(species)' (columns 0:3)
  (to view full data, use np.asarray(this_obj))
</code></pre><h1 id="标签特征二元化"><a class="anchor" href="#标签特征二元化">#</a> 标签特征二元化</h1><p>在这个主题中，我们将用另一种方式来演示分类变量。有些时候只有一两个分类特征是重要的，这时就要避免多余的维度，如果有多个分类变量就有可能会出现这些多余的维度。</p><h2 id="getting-ready-6"><a class="anchor" href="#getting-ready-6">#</a> Getting ready</h2><p>处理分类变量还有另一种方法，不需要通过 <code>OneHotEncoder</code> ，我们可以用 <code>LabelBinarizer</code> 。这是一个阈值与分类变量组合的方法。演示其用法之前，让我们加载 <code>iris</code> 数据集：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets <span class="token keyword">as</span> d</pre></td></tr><tr><td data-num="2"></td><td><pre>iris <span class="token operator">=</span> d<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>target <span class="token operator">=</span> iris<span class="token punctuation">.</span>target</pre></td></tr></table></figure><h2 id="how-to-do-it-6"><a class="anchor" href="#how-to-do-it-6">#</a> How to do it...</h2><p>导入 <code>LabelBinarizer()</code> 创建一个对象：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> LabelBinarizer</pre></td></tr><tr><td data-num="2"></td><td><pre>label_binarizer <span class="token operator">=</span> LabelBinarizer<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>现在，将因变量的值转换成一个新的特征向量：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>new_target <span class="token operator">=</span> label_binarizer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>target<span class="token punctuation">)</span></pre></td></tr></table></figure><p>让我们看看 <code>new_target</code> 和 <code>label_binarizer</code> 对象的结果：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>new_target<span class="token punctuation">.</span>shape</pre></td></tr></table></figure><pre><code>(150, 3)
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>new_target<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[1, 0, 0],
       [1, 0, 0],
       [1, 0, 0],
       [1, 0, 0],
       [1, 0, 0]])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>new_target<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">:</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[0, 0, 1],
       [0, 0, 1],
       [0, 0, 1],
       [0, 0, 1],
       [0, 0, 1]])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>label_binarizer<span class="token punctuation">.</span>classes_</pre></td></tr></table></figure><pre><code>array([0, 1, 2])
</code></pre><h2 id="how-it-works-6"><a class="anchor" href="#how-it-works-6">#</a> How it works...</h2><p><code>iris</code> 的因变量基数为 <code>3</code> ，就是说有三种值。当 <code>LabelBinarizer</code> 将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.76666em;vertical-align:-.08333em"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> 向量转换成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">N \times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.76666em;vertical-align:-.08333em"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07153em">C</span></span></span></span> 矩阵时，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07153em">C</span></span></span></span> 就是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N \times 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.76666em;vertical-align:-.08333em"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span> 向量的基数。需要注意的是，当 <code>label_binarizer</code> 处理因变量之后，再转换基数以外的值都是 <code>[0,0,0]</code> ：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>label_binarizer<span class="token punctuation">.</span>transform<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([[0, 0, 0]])
</code></pre><h2 id="theres-more-5"><a class="anchor" href="#theres-more-5">#</a> There's more...</h2><p>0 和 1 并不一定都是表示因变量中的阳性和阴性实例。例如，如果我们需要用 <code>1000</code> 表示阳性值，用 <code>-1000</code> 表示阴性值，我们可以用 <code>label_binarizer</code> 处理：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>label_binarizer <span class="token operator">=</span> LabelBinarizer<span class="token punctuation">(</span>neg_label<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1000</span><span class="token punctuation">,</span> pos_label<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>label_binarizer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>target<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 1000, -1000, -1000],
       [ 1000, -1000, -1000],
       [ 1000, -1000, -1000],
       [ 1000, -1000, -1000],
       [ 1000, -1000, -1000]])
</code></pre><blockquote><p>阳性和阴性值的唯一限制是，它们必须为整数。</p></blockquote><h1 id="处理缺失值"><a class="anchor" href="#处理缺失值">#</a> 处理缺失值</h1><p>实践中数值计算不可或缺，好在有很多方法可用，这个主题将介绍其中一些。不过，这些方法未必能解决你的问题。</p><p>scikit-learn 有一些常见的计算方法，它可以对现有数据进行变换填补 <code>NA</code> 值。但是，如果数据集中的缺失值是有意而为之的 —— 例如，服务器响应时间超过 100ms—— 那么更合适的方法是用其他包解决，像处理贝叶斯问题的 PyMC，处理风险模型的 lifelines，或者自己设计一套方法。</p><h2 id="getting-ready-7"><a class="anchor" href="#getting-ready-7">#</a> Getting ready</h2><p>处理缺失值的第一步是创建缺失值。Numpy 可以很方便的实现：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="3"></td><td><pre>iris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>iris_X <span class="token operator">=</span> iris<span class="token punctuation">.</span>data</pre></td></tr><tr><td data-num="5"></td><td><pre>masking_array <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>binomial<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">.25</span><span class="token punctuation">,</span> iris_X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">bool</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>iris_X<span class="token punctuation">[</span>masking_array<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>nan</pre></td></tr></table></figure><p>让我们看看这几行代码，Numpy 和平时用法不太一样，这里是在数组中用了一个数组作为索引。为了创建了随机的缺失值，先创建一个随机布尔值数组，其形状和 <code>iris_X</code> 数据集的维度相同。然后，根据布尔值数组分配缺失值。因为每次运行都是随机数据，所以 <code>masking_array</code> 每次都会不同。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>masking_array<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[False,  True, False, False],
       [False,  True, False, False],
       [False, False, False, False],
       [ True, False, False,  True],
       [False, False,  True, False]], dtype=bool)
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>iris_X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 5.1,  nan,  1.4,  0.2],
       [ 4.9,  nan,  1.4,  0.2],
       [ 4.7,  3.2,  1.3,  0.2],
       [ nan,  3.1,  1.5,  nan],
       [ 5. ,  3.6,  nan,  0.2]])
</code></pre><h2 id="how-to-do-it-7"><a class="anchor" href="#how-to-do-it-7">#</a> How to do it...</h2><p>本书贯穿始终的一条原则（由于 scikit-learn 的存在）就是那些拟合与转换数据集的类都是可用的，可以在其他数据集中继续使用。具体演示如下所示：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> preprocessing</pre></td></tr><tr><td data-num="2"></td><td><pre>impute <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>Imputer<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>iris_X_prime <span class="token operator">=</span> impute<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris_X<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>iris_X_prime<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 5.1       ,  3.05221239,  1.4       ,  0.2       ],
       [ 4.9       ,  3.05221239,  1.4       ,  0.2       ],
       [ 4.7       ,  3.2       ,  1.3       ,  0.2       ],
       [ 5.86306306,  3.1       ,  1.5       ,  1.21388889],
       [ 5.        ,  3.6       ,  3.82685185,  0.2       ]])
</code></pre><p>注意 <code>[3,0]</code> 位置的不同：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>iris_X_prime<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>5.8630630630630645
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>iris_X<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>nan
</code></pre><h2 id="how-it-works-7"><a class="anchor" href="#how-it-works-7">#</a> How it works...</h2><p>上面的计算可以通过不同的方法实现。默认是均值 <code>mean</code> ，一共是三种：</p><ul><li>均值 <code>mean</code> （默认方法）</li><li>中位数 <code>median</code></li><li>众数 <code>most_frequent</code></li></ul><p>scikit-learn 会用指定的方法计算数据集中的每个缺失值，然后把它们填充好。</p><p>例如，用 <code>median</code> 方法重新计算 <code>iris_X</code> ，重新初始化 <code>impute</code> 即可：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>impute <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>Imputer<span class="token punctuation">(</span>strategy<span class="token operator">=</span><span class="token string">'median'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>iris_X_prime <span class="token operator">=</span> impute<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris_X<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>iris_X_prime<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 5.1 ,  3.  ,  1.4 ,  0.2 ],
       [ 4.9 ,  3.  ,  1.4 ,  0.2 ],
       [ 4.7 ,  3.2 ,  1.3 ,  0.2 ],
       [ 5.8 ,  3.1 ,  1.5 ,  1.3 ],
       [ 5.  ,  3.6 ,  4.45,  0.2 ]])
</code></pre><p>如果数据有缺失值，后面计算过程中可能会出问题。例如，在 * How to do it...* 一节里面， <code>np.nan</code> 作为默认缺失值，但是缺失值有很多表现形式。有时用 <code>-1</code> 表示。为了处理这些缺失值，可以在方法中指定那些值是缺失值。方法默认缺失值表现形式是 <code>Nan</code> ，就是 <code>np.nan</code> 的值。</p><p>假设我们将 <code>iris_X</code> 的缺失值都用 <code>-1</code> 表示。看着很奇怪，但是 <code>iris</code> 数据集的度量值不可能是负数，因此用 <code>-1</code> 表示缺失值完全合理：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>iris_X<span class="token punctuation">[</span>np<span class="token punctuation">.</span>isnan<span class="token punctuation">(</span>iris_X<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span></pre></td></tr><tr><td data-num="2"></td><td><pre>iris_X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 5.1, -1. ,  1.4,  0.2],
       [ 4.9, -1. ,  1.4,  0.2],
       [ 4.7,  3.2,  1.3,  0.2],
       [-1. ,  3.1,  1.5, -1. ],
       [ 5. ,  3.6, -1. ,  0.2]])
</code></pre><p>填充这些缺失值也很简单：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>impute <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>Imputer<span class="token punctuation">(</span>missing_values<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>iris_X_prime <span class="token operator">=</span> impute<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris_X<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>iris_X_prime<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 5.1       ,  3.05221239,  1.4       ,  0.2       ],
       [ 4.9       ,  3.05221239,  1.4       ,  0.2       ],
       [ 4.7       ,  3.2       ,  1.3       ,  0.2       ],
       [ 5.86306306,  3.1       ,  1.5       ,  1.21388889],
       [ 5.        ,  3.6       ,  3.82685185,  0.2       ]])
</code></pre><h2 id="theres-more-6"><a class="anchor" href="#theres-more-6">#</a> There's more...</h2><p>pandas 库也可以处理缺失值，而且更加灵活，但是重用性较弱：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd</pre></td></tr><tr><td data-num="2"></td><td><pre>iris_X<span class="token punctuation">[</span>masking_array<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>nan</pre></td></tr><tr><td data-num="3"></td><td><pre>iris_df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>iris_X<span class="token punctuation">,</span> columns<span class="token operator">=</span>iris<span class="token punctuation">.</span>feature_names<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>iris_df<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>iris_df<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'sepal length (cm)'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>0    5.100000
1    4.900000
2    4.700000
3    5.863063
4    5.000000
Name: sepal length (cm), dtype: float64
</code></pre><p>其灵活性在于， <code>fillna</code> 可以填充任意统计参数值：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>iris_df<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span>iris_df<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'sepal length (cm)'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>head<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>0    5.1
1    4.9
2    4.7
3    7.9
4    5.0
Name: sepal length (cm), dtype: float64
</code></pre><h1 id="用管线命令处理多个步骤"><a class="anchor" href="#用管线命令处理多个步骤">#</a> 用管线命令处理多个步骤</h1><p>管线命令不经常用，但是很有用。它们可以把多个步骤组合成一个对象执行。这样可以更方便灵活地调节和控制整个模型的配置，而不只是一个一个步骤调节。</p><h2 id="getting-ready-8"><a class="anchor" href="#getting-ready-8">#</a> Getting ready</h2><p>这是我们把多个数据处理步骤组合成一个对象的第一部分。在 scikit-learn 里称为 <code>pipeline</code> 。这里我们首先通过计算处理缺失值；然后将数据集调整为均值为 0，标准差为 1 的标准形。</p><p>让我们创建一个有缺失值的数据集，然后再演示 <code>pipeline</code> 的用法：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="3"></td><td><pre>mat <span class="token operator">=</span> datasets<span class="token punctuation">.</span>make_spd_matrix<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>masking_array <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>binomial<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">.1</span><span class="token punctuation">,</span> mat<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">bool</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>mat<span class="token punctuation">[</span>masking_array<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>nan</pre></td></tr><tr><td data-num="6"></td><td><pre>mat<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 1.05419595,  1.42287309,  0.02368264, -0.8505244 ],
       [ 1.42287309,  5.09704588,         nan, -2.46408728],
       [ 0.02368264,  0.03614203,  0.63317494,  0.09792298],
       [-0.8505244 , -2.46408728,  0.09792298,  2.04110849]])
</code></pre><h2 id="how-to-do-it-8"><a class="anchor" href="#how-to-do-it-8">#</a> How to do it...</h2><p>如果不用管线命令，我们可能会这样实现：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> preprocessing</pre></td></tr><tr><td data-num="2"></td><td><pre>impute <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>Imputer<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>scaler <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>mat_imputed <span class="token operator">=</span> impute<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>mat<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>mat_imputed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 1.05419595,  1.42287309,  0.02368264, -0.8505244 ],
       [ 1.42287309,  5.09704588,  0.09560571, -2.46408728],
       [ 0.02368264,  0.03614203,  0.63317494,  0.09792298],
       [-0.8505244 , -2.46408728,  0.09792298,  2.04110849]])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>mat_imp_and_scaled <span class="token operator">=</span> scaler<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>mat_imputed<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>mat_imp_and_scaled<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[  1.09907483e+00,   2.62635324e-01,  -3.88958755e-01,
         -4.80451718e-01],
       [  1.63825210e+00,   2.01707858e+00,  -7.50508486e-17,
         -1.80311396e+00],
       [ -4.08014393e-01,  -3.99538476e-01,   2.90716556e+00,
          2.97005140e-01],
       [ -1.68651124e+00,  -1.59341549e+00,   1.25317595e-02,
          1.88986410e+00]])
</code></pre><p>现在我们用 <code>pipeline</code> 来演示：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> pipeline</pre></td></tr><tr><td data-num="2"></td><td><pre>pipe <span class="token operator">=</span> pipeline<span class="token punctuation">.</span>Pipeline<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'impute'</span><span class="token punctuation">,</span> impute<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'scaler'</span><span class="token punctuation">,</span> scaler<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>我们看看 <code>pipe</code> 的内容。和前面介绍一致，管线命令定义了处理步骤：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>pipe</pre></td></tr></table></figure><pre><code>Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True))])
</code></pre><p>然后在调用 <code>pipe</code> 的 <code>fit_transform</code> 方法，就可以把多个步骤组合成一个对象了：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>new_mat <span class="token operator">=</span> pipe<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>mat<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>new_mat<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[  1.09907483e+00,   2.62635324e-01,  -3.88958755e-01,
         -4.80451718e-01],
       [  1.63825210e+00,   2.01707858e+00,  -7.50508486e-17,
         -1.80311396e+00],
       [ -4.08014393e-01,  -3.99538476e-01,   2.90716556e+00,
          2.97005140e-01],
       [ -1.68651124e+00,  -1.59341549e+00,   1.25317595e-02,
          1.88986410e+00]])
</code></pre><p>可以用 Numpy 验证一下结果：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>array_equal<span class="token punctuation">(</span>new_mat<span class="token punctuation">,</span> mat_imp_and_scaled<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>True
</code></pre><p>完全正确！本书后面的主题中，我们会进一步展示管线命令的威力。不仅可以用于预处理步骤中，在降维、算法拟合中也可以很方便的使用。</p><h2 id="how-it-works-8"><a class="anchor" href="#how-it-works-8">#</a> How it works...</h2><p>前面曾经提到过，每个 scikit-learn 的算法接口都类似。 <code>pipeline</code> 最重要的函数也不外乎下面三个：</p><ul><li><code>fit</code></li><li><code>transform</code></li><li><code>fit_transform</code></li></ul><p>具体来说，如果管线命令有 <code>N</code> 个对象，前 <code>N-1</code> 个对象必须实现 <code>fit</code> 和 <code>transform</code> ，第 <code>N</code> 个对象至少实现 <code>fit</code> 。否则就会出现错误。</p><p>如果这些条件满足，管线命令就会运行，但是不一定每个方法都可以。例如， <code>pipe</code> 有个 <code>inverse_transform</code> 方法就是这样。因为由于计算步骤没有 <code>inverse_transform</code> 方法，一运行就有错误：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>pipe<span class="token punctuation">.</span>inverse_transform<span class="token punctuation">(</span>new_mat<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-12-62edd2667cae&gt; in &lt;module&gt;()
----&gt; 1 pipe.inverse_transform(new_mat)


d:\programfiles\Miniconda3\lib\site-packages\sklearn\utils\metaestimators.py in &lt;lambda&gt;(*args, **kwargs)
     35             self.get_attribute(obj)
     36         # lambda, but not partial, allows help() to work with update_wrapper
---&gt; 37         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
     38         # update the docstring of the returned function
     39         update_wrapper(out, self.fn)


d:\programfiles\Miniconda3\lib\site-packages\sklearn\pipeline.py in inverse_transform(self, X)
    265         Xt = X
    266         for name, step in self.steps[::-1]:
--&gt; 267             Xt = step.inverse_transform(Xt)
    268         return Xt
    269 


AttributeError: 'Imputer' object has no attribute 'inverse_transform'
</code></pre><p>但是， <code>scalar</code> 对象可以正常运行：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>scaler<span class="token punctuation">.</span>inverse_transform<span class="token punctuation">(</span>new_mat<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 1.05419595,  1.42287309,  0.02368264, -0.8505244 ],
       [ 1.42287309,  5.09704588,  0.09560571, -2.46408728],
       [ 0.02368264,  0.03614203,  0.63317494,  0.09792298],
       [-0.8505244 , -2.46408728,  0.09792298,  2.04110849]])
</code></pre><p>只要把管线命令设置好，它就会如愿运行。它就是一组 <code>for</code> 循环，对每个步骤执行 <code>fit</code> 和 <code>transform</code> ，然后把结果传递到下一个变换操作中。</p><p>使用管线命令的理由主要有两点：</p><ul><li>首先是方便。代码会简洁一些，不需要重复调用 <code>fit</code> 和 <code>transform</code> 。</li><li>其次，也是更重要的作用，就是使用交叉验证。模型可以变得很复杂。如果管线命令中的一个步骤调整了参数，那么它们必然需要重新测试；测试一个步骤参数的代码管理成本是很低的。但是，如果测试 5 个步骤的全部参数会变都很复杂。管线命令可以缓解这些负担。</li></ul><h1 id="用主成分分析降维"><a class="anchor" href="#用主成分分析降维">#</a> 用主成分分析降维</h1><p>现在是时候升一级了！主成分分析（Principal component analysis，PCA）是本书介绍的第一个高级技术。到目前为止都是些简单的统计学知识，而 PCA 将统计学和线性代数组合起来实现降维，堪称简单模型的杀手锏。</p><h2 id="getting-ready-9"><a class="anchor" href="#getting-ready-9">#</a> Getting ready</h2><p>PCA 是 scikit-learn 的一个分解模块。还有一些分解模块后面会介绍。让我们用 <code>iris</code> 数据集演示一下，你也可以用自己的数据集：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre>iris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>iris_X <span class="token operator">=</span> iris<span class="token punctuation">.</span>data</pre></td></tr></table></figure><h2 id="how-to-do-it-9"><a class="anchor" href="#how-to-do-it-9">#</a> How to do it...</h2><p>首先导入分解模块：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> decomposition</pre></td></tr></table></figure><p>然后，初始化一个 PCA 对象：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>pca <span class="token operator">=</span> decomposition<span class="token punctuation">.</span>PCA<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>pca</pre></td></tr></table></figure><pre><code>PCA(copy=True, n_components=None, whiten=False)
</code></pre><p>和 scikit-learn 其他对象相比，PCA 的参数很少。这样 PCA 对象就创建了，下面用 <code>fit_transform</code> 处理 <code>iris_X</code> 数据：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>iris_pca <span class="token operator">=</span> pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris_X<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>iris_pca<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ -2.68420713e+00,  -3.26607315e-01,   2.15118370e-02,
          1.00615724e-03],
       [ -2.71539062e+00,   1.69556848e-01,   2.03521425e-01,
          9.96024240e-02],
       [ -2.88981954e+00,   1.37345610e-01,  -2.47092410e-02,
          1.93045428e-02],
       [ -2.74643720e+00,   3.11124316e-01,  -3.76719753e-02,
         -7.59552741e-02],
       [ -2.72859298e+00,  -3.33924564e-01,  -9.62296998e-02,
         -6.31287327e-02]])
</code></pre><p>这样 PCA 就完成了，我们可以看看降维的效果：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>pca<span class="token punctuation">.</span>explained_variance_ratio_</pre></td></tr></table></figure><pre><code>array([ 0.92461621,  0.05301557,  0.01718514,  0.00518309])
</code></pre><h2 id="how-it-works-9"><a class="anchor" href="#how-it-works-9">#</a> How it works...</h2><p>PCA 是在数据分析中有一般性的数学定义和具体的应用场景。PCA 用正交向量集表示原始数据集。</p><p>通常，PCA 将原始数据集映射到新的空间中，里面每个列向量都是彼此正交的。从数据分析的视角看，PCA 将数据集的协方差矩阵变换成若干能够 “解释” 一定比例变量的列向量。例如，在 <code>iris</code> 数据集中，92.5% 的变量可以由第一个主成份表示。</p><p>数据分析里面维度多会导致维度灾难，因此降维至关重要。通常算法处理高维训练集时会出现拟合过度（overfit）的情况，于是难以把握测试集的一般性特征。如果数据集的真实结构可以用更少的维度表示，那么通常都值得一试。</p><p>为了演示这点，我们用 PCA 将 <code>iris</code> 数据集转换成二维数据。 <code>iris</code> 数据集用全部的维度通常可以很好的分类：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>pca <span class="token operator">=</span> decomposition<span class="token punctuation">.</span>PCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>iris_X_prime <span class="token operator">=</span> pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris_X<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>iris_X_prime<span class="token punctuation">.</span>shape</pre></td></tr></table></figure><pre><code>(150, 2)
</code></pre><p>我们的矩阵现在是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>150</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">150 \times 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mord">5</span><span class="mord">0</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span></span></span></span>，不是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>150</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">150 \times 4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72777em;vertical-align:-.08333em"></span><span class="mord">1</span><span class="mord">5</span><span class="mord">0</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">4</span></span></span></span> 了。二维变量更容易可视化：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="3"></td><td><pre>f <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>ax <span class="token operator">=</span> f<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>iris_X_prime<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> iris_X_prime<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token operator">=</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"PCA 2 Components"</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>&lt;matplotlib.text.Text at 0x84571d0&gt;
</code></pre><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-9-1.png" alt="png"></p><p>把数据集降成二维之后还是分离特征依然保留。我们可以查看这二维数据保留了多少变量信息：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>pca<span class="token punctuation">.</span>explained_variance_ratio_<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>0.97763177502480336
</code></pre><h2 id="theres-more-7"><a class="anchor" href="#theres-more-7">#</a> There's more...</h2><p>PCA 对象还可以一开始设置解释变量的比例。例如，如果我们想介绍 98% 的变量，PCA 对象就可以这样创建：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>pca <span class="token operator">=</span> decomposition<span class="token punctuation">.</span>PCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">.98</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>iris_X_prime <span class="token operator">=</span> pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris_X<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>pca<span class="token punctuation">.</span>explained_variance_ratio_<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>0.99481691454981014
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>iris_X_prime<span class="token punctuation">.</span>shape</pre></td></tr></table></figure><pre><code>(150, 3)
</code></pre><p>由于我们想比二维主成份解释更多的变量，第三维就需要了。</p><h1 id="用因子分析降维"><a class="anchor" href="#用因子分析降维">#</a> 用因子分析降维</h1><p>因子分析（factor analysis）是另一种降维方法。与 PCA 不同的是，因子分析有假设而 PCA 没有假设。因子分析的基本假设是有一些隐藏特征与数据集的特征相关。</p><p>这个主题将浓缩（boil down）样本数据集的显性特征，尝试像理解因变量一样地理解自变量之间的隐藏特征。</p><h2 id="getting-ready-10"><a class="anchor" href="#getting-ready-10">#</a> Getting ready</h2><p>让我们再用 <code>iris</code> 数据集来比较 PCA 与因子分析，首先加载因子分析类：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre>iris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> FactorAnalysis</pre></td></tr></table></figure><h2 id="how-to-do-it-10"><a class="anchor" href="#how-to-do-it-10">#</a> How to do it...</h2><p>从编程角度看，两种方法没啥区别：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>fa <span class="token operator">=</span> FactorAnalysis<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>iris_two_dim <span class="token operator">=</span> fa<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris<span class="token punctuation">.</span>data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>iris_two_dim<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[-1.33125848,  0.55846779],
       [-1.33914102, -0.00509715],
       [-1.40258715, -0.307983  ],
       [-1.29839497, -0.71854288],
       [-1.33587575,  0.36533259]])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="3"></td><td><pre>f <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>ax <span class="token operator">=</span> f<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>iris_two_dim<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> iris_two_dim<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token operator">=</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Factor Analysis 2 Components"</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>&lt;matplotlib.text.Text at 0x8875ba8&gt;
</code></pre><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-10-1.png" alt="png"></p><p>由于因子分析是一种概率性的转换方法，我们可以通过不同的角度来观察，例如模型观测值的对数似然估计值，通过模型比较对数似然估计值会更好。</p><p>因子分析也有不足之处。由于你不是通过拟合模型直接预测结果，拟合模型只是一个中间步骤。这本身并非坏事，但是训练实际模型时误差就会产生。</p><h2 id="how-it-works-10"><a class="anchor" href="#how-it-works-10">#</a> How it works...</h2><p>因子分析与前面介绍的 PCA 类似。但两者有一个不同之处。PCA 是通过对数据进行线性变换获取一个能够解释数据变量的主成分向量空间，这个空间中的每个主成分向量都是正交的。你可以把 PCA 看成是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span> 维数据集降维成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span> 维，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>&lt;</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">M \lt N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.72243em;vertical-align:-.0391em"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span>。</p><p>而因子分析的基本假设是，有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span> 个重要特征和它们的线性组合（加噪声），能够构成原始的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span> 维数据集。也就是说，你不需要指定结果变量（就是最终生成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span></span></span></span> 维），而是要指定数据模型的因子数量（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span> 个因子）。</p><h1 id="用核pca实现非线性降维"><a class="anchor" href="#用核pca实现非线性降维">#</a> 用核 PCA 实现非线性降维</h1><p>由于大多数统计方法最开始都是线性的，所以，想解决非线性问题，就需要做一些调整。PCA 也是一种线性变换。本主题将首先介绍它的非线性形式，然后介绍如何降维。</p><h2 id="getting-ready-11"><a class="anchor" href="#getting-ready-11">#</a> Getting ready</h2><p>如果数据都是线性的，生活得多容易啊，可惜现实并非如此。核主成分分析（Kernel PCA）可以处理非线性问题。数据先通过核函数（kernel function）转换成一个新空间，然后再用 PCA 处理。</p><p>要理解核函数之前，建议先尝试如何生成一个能够通过核 PCA 里的核函数线性分割的数据集。下面我们用余弦核（cosine kernel）演示。这个主题比前面的主题多一些理论。</p><h2 id="how-to-do-it-11"><a class="anchor" href="#how-to-do-it-11">#</a> How to do it...</h2><p>余弦核可以用来比例样本空间中两个样本向量的夹角。当向量的大小（magnitude）用传统的距离度量不合适的时候，余弦核就有用了。</p><p>向量夹角的余弦公式如下：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>A</mi><mo>⋅</mo><mi>B</mi></mrow><mrow><mrow><mo fence="true">∥</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>A</mi></mstyle></mtd></mtr></mtable><mo fence="true">∥</mo></mrow><mrow><mo fence="true">∥</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>B</mi></mstyle></mtd></mtr></mtable><mo fence="true">∥</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex">cos(\theta)=\frac {A \cdot B} { {\begin{Vmatrix} A \end{Vmatrix}} {\begin{Vmatrix} B \end{Vmatrix}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.4683200000000003em;vertical-align:-1.10799em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em"><span style="top:-2.24202em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8500000000000001em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000000000000003em"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span></span></span><span class="mord"><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8500000000000001em"><span style="top:-3.01em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000000000000003em"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8679800000000001em"><span style="top:-2.2559899999999997em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.26698em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span><span style="top:-2.86798em"><span class="pstrut" style="height:2.606em"></span><span class="delimsizinginner delim-size1"><span>∥</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.35000999999999993em"><span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathnormal" style="margin-right:.05017em">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.10799em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal">A</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.05017em">B</span></span></span></span> 夹角的余弦是两向量点积除以两个向量各自的 L2 范数。向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal">A</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.05017em">B</span></span></span></span> 的大小不会影响余弦值。</p><p>让我们生成一些数据来演示一下用法。首先，我们假设有两个不同的过程数据（process），称为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal">A</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.05017em">B</span></span></span></span>：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre>A1_mean <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="3"></td><td><pre>A1_cov <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">.99</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="4"></td><td><pre>A1 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>multivariate_normal<span class="token punctuation">(</span>A1_mean<span class="token punctuation">,</span> A1_cov<span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>A2_mean <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="6"></td><td><pre>A2_cov <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">.99</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="7"></td><td><pre>A2 <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>multivariate_normal<span class="token punctuation">(</span>A2_mean<span class="token punctuation">,</span> A2_cov<span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>A <span class="token operator">=</span> np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span><span class="token punctuation">(</span>A1<span class="token punctuation">,</span> A2<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>B_mean <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="10"></td><td><pre>B_cov <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">.5</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">.9</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="11"></td><td><pre>B <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>multivariate_normal<span class="token punctuation">(</span>B_mean<span class="token punctuation">,</span> B_cov<span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="3"></td><td><pre>f <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>ax <span class="token operator">=</span> f<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"$A$ and $B$ processes"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> A<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>A2<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> A2<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>B<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> B<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>&lt;matplotlib.collections.PathCollection at 0x73cd128&gt;
</code></pre><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-11-1.png" alt="png"></p><p>上图看起来明显是两个不同的过程数据，但是用一超平面分割它们很难。因此，我们用前面介绍带余弦核的核 PCA 来处理：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> KernelPCA</pre></td></tr><tr><td data-num="2"></td><td><pre>kpca <span class="token operator">=</span> KernelPCA<span class="token punctuation">(</span>kernel<span class="token operator">=</span><span class="token string">'cosine'</span><span class="token punctuation">,</span> n_components<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>AB <span class="token operator">=</span> np<span class="token punctuation">.</span>vstack<span class="token punctuation">(</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>AB_transformed <span class="token operator">=</span> kpca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>AB<span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>A_color <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'r'</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token builtin">len</span><span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>B_color <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'b'</span><span class="token punctuation">]</span><span class="token operator">*</span><span class="token builtin">len</span><span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>colors <span class="token operator">=</span> np<span class="token punctuation">.</span>hstack<span class="token punctuation">(</span><span class="token punctuation">(</span>A_color<span class="token punctuation">,</span> B_color<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>f <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax <span class="token operator">=</span> f<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Cosine KPCA 1 Dimension"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>AB_transformed<span class="token punctuation">,</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>AB_transformed<span class="token punctuation">)</span><span class="token punctuation">,</span> color<span class="token operator">=</span>colors<span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-11-2.png" alt="png"></p><p>用带余弦核的核 PCA 处理后，数据集变成了一维。如果用 PCA 处理就是这样：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> PCA</pre></td></tr><tr><td data-num="2"></td><td><pre>pca <span class="token operator">=</span> PCA<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>AB_transformed_Reg <span class="token operator">=</span> pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>AB<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>f <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax <span class="token operator">=</span> f<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"PCA 1 Dimension"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>AB_transformed_Reg<span class="token punctuation">,</span> np<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>AB_transformed_Reg<span class="token punctuation">)</span><span class="token punctuation">,</span> color<span class="token operator">=</span>colors<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>&lt;matplotlib.collections.PathCollection at 0x7c764a8&gt;
</code></pre><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-11-3.png" alt="png"></p><p>很明显，核 PCA 降维效果更好。</p><h2 id="how-it-works-11"><a class="anchor" href="#how-it-works-11">#</a> How it works...</h2><p>scikit-learn 提供了几种像余弦核那样的核函数，也可以写自己的核函数。默认的函数有：</p><ul><li>线性函数（linear）（默认值）</li><li>多项式函数（poly）</li><li>径向基函数（rbf，radial basis function）</li><li>S 形函数（sigmoid）</li><li>余弦函数（cosine）</li><li>用户自定义函数（precomputed）</li></ul><p>还有一些因素会影响核函数的选择。例如， <code>degree</code> 参数可以设置 <code>poly</code> ， <code>rbf</code> 和 <code>sigmoid</code> 核函数的角度；而 <code>gamma</code> 会影响 <code>rbf</code> 和 <code>poly</code> 核，更多详情请查看<a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html"> <code>KernelPCA</code> 文档</a>。</p><p>后面关于支持向量机（SVM）的主题中将会进一步介绍 <code>rbf</code> 核函数。</p><p>需要注意的是：核函数处理非线性分离效果很好，但是一不小心就可能导致拟合过度。</p><h1 id="用截断奇异值分解降维"><a class="anchor" href="#用截断奇异值分解降维">#</a> 用截断奇异值分解降维</h1><p>截断奇异值分解（Truncated singular value decomposition，TSVD）是一种矩阵因式分解（factorization）技术，将矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">M</span></span></span></span> 分解成<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">U</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord">Σ</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span>。它与 PCA 很像，只是 SVD 分解是在数据矩阵上进行，而 PCA 是在数据的协方差矩阵上进行。通常，SVD 用于发现矩阵的主成份。</p><h2 id="getting-ready-12"><a class="anchor" href="#getting-ready-12">#</a> Getting ready</h2><p>TSVD 与一般 SVD 不同的是它可以产生一个指定维度的分解矩阵。例如，有一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span> 矩阵，通过 SVD 分解后仍然是一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.66666em;vertical-align:-.08333em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span> 矩阵，而 TSVD 可以生成指定维度的矩阵。这样就可以实现降维了。</p><p>这里我们还用 <code>iris</code> 数据集来演示 TSVD：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris</pre></td></tr><tr><td data-num="2"></td><td><pre>iris <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>iris_data <span class="token operator">=</span> iris<span class="token punctuation">.</span>data</pre></td></tr></table></figure><h2 id="how-to-do-it-12"><a class="anchor" href="#how-to-do-it-12">#</a> How to do it...</h2><p>TSVD 对象的用法和其他对象类似。首先导入需要的类，初始化，然后拟合：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> TruncatedSVD</pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>svd <span class="token operator">=</span> TruncatedSVD<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>iris_transformed <span class="token operator">=</span> svd<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris_data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>iris_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 5.1,  3.5,  1.4,  0.2],
       [ 4.9,  3. ,  1.4,  0.2],
       [ 4.7,  3.2,  1.3,  0.2],
       [ 4.6,  3.1,  1.5,  0.2],
       [ 5. ,  3.6,  1.4,  0.2]])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>iris_transformed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 5.91220352, -2.30344211],
       [ 5.57207573, -1.97383104],
       [ 5.4464847 , -2.09653267],
       [ 5.43601924, -1.87168085],
       [ 5.87506555, -2.32934799]])
</code></pre><p>最终结果如下图所示：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="3"></td><td><pre>f <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>ax <span class="token operator">=</span> f<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>iris_transformed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> iris_transformed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token operator">=</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Truncated SVD, 2 Components"</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>&lt;matplotlib.text.Text at 0x8600be0&gt;
</code></pre><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-12-1.png" alt="png"></p><h2 id="how-it-works-12"><a class="anchor" href="#how-it-works-12">#</a> How it works...</h2><p>现在我们演示了 scikit-learn 的 <code>TruncatedSVD</code> 模块，让我们看看只用 <code>scipy</code> 学习一些细节。</p><p>首先，我们用 <code>scipy</code> 的 <code>linalg</code> 处理 SVD：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> scipy<span class="token punctuation">.</span>linalg <span class="token keyword">import</span> svd</pre></td></tr><tr><td data-num="3"></td><td><pre>D <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>D</pre></td></tr></table></figure><pre><code>array([[1, 2],
       [1, 3],
       [1, 4]])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>U<span class="token punctuation">,</span> S<span class="token punctuation">,</span> V <span class="token operator">=</span> svd<span class="token punctuation">(</span>D<span class="token punctuation">,</span> full_matrices<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>U<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> S<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> V<span class="token punctuation">.</span>shape</pre></td></tr></table></figure><pre><code>((3, 2), (2,), (2, 2))
</code></pre><p>我们可以根据 SVD 的定义，用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">U</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.05764em">S</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.22222em">V</span></span></span></span> 还原矩阵<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">D</span></span></span></span>：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>S<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([[ 5.64015854,  0.        ],
       [ 0.        ,  0.43429448]])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>U<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>S<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> V<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([[ 1.,  2.],
       [ 1.,  3.],
       [ 1.,  4.]])
</code></pre><p><code>TruncatedSVD</code> 返回的矩阵是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">U</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.05764em">S</span></span></span></span> 的点积。如果我们想模拟 TSVD，我们就去掉最新奇异值和对于<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.10903em">U</span></span></span></span> 的列向量。例如，我们想要一个主成份，可以这样：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>new_S <span class="token operator">=</span> S<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="2"></td><td><pre>new_U <span class="token operator">=</span> U<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span></pre></td></tr><tr><td data-num="3"></td><td><pre>new_U<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>new_S<span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>array([-2.20719466, -3.16170819, -4.11622173])
</code></pre><p>一般情况下，如果我们想要截断维度<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.61508em;vertical-align:0"></span><span class="mord mathnormal">t</span></span></span></span>，那么我们就去掉<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>−</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">N-t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.76666em;vertical-align:-.08333em"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.61508em;vertical-align:0"></span><span class="mord mathnormal">t</span></span></span></span> 个奇异值。</p><h2 id="theres-more-8"><a class="anchor" href="#theres-more-8">#</a> There's more...</h2><p><code>TruncatedSVD</code> 还有一些细节需要注意。</p><h3 id="符号翻转sign-flipping"><a class="anchor" href="#符号翻转sign-flipping">#</a> 符号翻转（Sign flipping）</h3><p><code>TruncatedSVD</code> 有个 “陷阱”。随着随机数生成器状态的变化， <code>TruncatedSVD</code> 连续地拟合会改变输出的符合。为了避免这个问题，建议只用 <code>TruncatedSVD</code> 拟合一次，然后用其他变换。这正是管线命令的另一个用处。</p><p>要避免这种情况，可以这样：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>tsvd <span class="token operator">=</span> TruncatedSVD<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>tsvd<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>iris_data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>tsvd<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>iris_data<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 5.91220352, -2.30344211],
       [ 5.57207573, -1.97383104],
       [ 5.4464847 , -2.09653267],
       [ 5.43601924, -1.87168085],
       [ 5.87506555, -2.32934799]])
</code></pre><h3 id="稀疏矩阵-2"><a class="anchor" href="#稀疏矩阵-2">#</a> 稀疏矩阵</h3><p><code>TruncatedSVD</code> 相比 PDA 的一个优势是 <code>TruncatedSVD</code> 可以操作 PDA 处理不了的矩阵。这是因为 PCA 必须计算协方差矩阵，需要在整个矩阵上操作，如果矩阵太大，计算资源可能会不够用。</p><h1 id="用字典学习分解法分类"><a class="anchor" href="#用字典学习分解法分类">#</a> 用字典学习分解法分类</h1><p>在这个主题中，我们将介绍一种可以用于分类的分解方法 —— 字典学习（Dictionary Learning），将数据集转换成一个稀疏的形式。</p><h2 id="getting-ready-13"><a class="anchor" href="#getting-ready-13">#</a> Getting ready</h2><p><code>DictionaryLearning</code> 方法的思想是把特征看作构成数据集的基础。首先我们导入 <code>iris</code> 数据集：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris</pre></td></tr><tr><td data-num="2"></td><td><pre>iris <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>iris_data <span class="token operator">=</span> iris<span class="token punctuation">.</span>data</pre></td></tr><tr><td data-num="4"></td><td><pre>iris_target <span class="token operator">=</span> iris<span class="token punctuation">.</span>target</pre></td></tr></table></figure><h2 id="how-to-do-it-13"><a class="anchor" href="#how-to-do-it-13">#</a> How to do it...</h2><p>首先，导入 <code>DictionaryLearning</code> ：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> DictionaryLearning</pre></td></tr></table></figure><p>然后用三个成分表示 <code>iris</code> 数据集中花的类型：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>dl <span class="token operator">=</span> DictionaryLearning<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>再用 <code>fit_transform</code> 转换其他数据，这样我们就可以对比训练前后的数据了：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>transformed <span class="token operator">=</span> dl<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>transformed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 0.        ,  6.34476574,  0.        ],
       [ 0.        ,  5.83576461,  0.        ],
       [ 0.        ,  6.32038375,  0.        ],
       [ 0.        ,  5.89318572,  0.        ],
       [ 0.        ,  5.45222715,  0.        ]])
</code></pre><p>我们可以可视化这个结果。注意，每个成分的值分别平行<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.03588em">y</span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.04398em">z</span></span></span></span> 三个轴，其他坐标都是 0；这就是稀疏性。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> mpl_toolkits<span class="token punctuation">.</span>mplot3d <span class="token keyword">import</span> Axes3D</pre></td></tr><tr><td data-num="2"></td><td><pre>colors <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token string">'rgb'</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>f <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>ax <span class="token operator">=</span> f<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">,</span> projection<span class="token operator">=</span><span class="token string">'3d'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Training Set"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>transformed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> transformed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> transformed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> color<span class="token operator">=</span>colors<span class="token punctuation">[</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-13-1.png" alt="png"></p><p>如果你细心看，还是会发现一些误差。有一个样本分错了类型，虽然一个错误并不是很严重。</p><p>下面，让我们用 <code>fit</code> 而不用 <code>fit_transform</code> 来训练数据集：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>transformed <span class="token operator">=</span> dl<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>iris_data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>colors <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token string">'rgb'</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>f <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>ax <span class="token operator">=</span> f<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span><span class="token number">111</span><span class="token punctuation">,</span> projection<span class="token operator">=</span><span class="token string">'3d'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Training Set"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>transformed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> transformed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> transformed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> color<span class="token operator">=</span>colors<span class="token punctuation">[</span>iris<span class="token punctuation">.</span>target<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-13-2.png" alt="png"></p><p>还是有一些分类错误的样本。如果你看看之前降维主题中的图，会发现绿色和蓝色两类数据有交叉部分。</p><h2 id="how-it-works-13"><a class="anchor" href="#how-it-works-13">#</a> How it works...</h2><p><code>DictionaryLearning</code> 具有信号处理和神经学领域的背景知识。其理念是某一时刻只有少数特征可以实现。因此， <code>DictionaryLearning</code> 在假设大多数特征都是 0 的情况下，尝试发现一个适当的数据表现形式。</p><h1 id="用管线命令连接多个转换方法"><a class="anchor" href="#用管线命令连接多个转换方法">#</a> 用管线命令连接多个转换方法</h1><p>下面，让我们用管线命令连接多个转换方法，来演示一个复杂点儿的例子。</p><h2 id="getting-ready-14"><a class="anchor" href="#getting-ready-14">#</a> Getting ready</h2><p>本主题将再度释放管线命令的光芒。之前我们用它处理缺失数据，只是牛刀小试罢了。下面我们用管线命令把多个预处理步骤连接起来处理，会非常方便。</p><p>首先，我们加载带缺失值的 <code>iris</code> 数据集：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_iris</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="3"></td><td><pre></pre></td></tr><tr><td data-num="4"></td><td><pre>iris <span class="token operator">=</span> load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>iris_data <span class="token operator">=</span> iris<span class="token punctuation">.</span>data</pre></td></tr><tr><td data-num="6"></td><td><pre></pre></td></tr><tr><td data-num="7"></td><td><pre>mask <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>binomial<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">.25</span><span class="token punctuation">,</span> iris_data<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">bool</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>iris_data<span class="token punctuation">[</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>nan</pre></td></tr><tr><td data-num="9"></td><td><pre></pre></td></tr><tr><td data-num="10"></td><td><pre>iris_data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ nan,  3.5,  1.4,  0.2],
       [ 4.9,  3. ,  1.4,  0.2],
       [ 4.7,  3.2,  1.3,  0.2],
       [ 4.6,  nan,  1.5,  nan],
       [ 5. ,  3.6,  1.4,  0.2]])
</code></pre><h2 id="how-to-do-it-14"><a class="anchor" href="#how-to-do-it-14">#</a> How to do it...</h2><p>本主题的目标是首先补全 <code>iris_data</code> 的缺失值，然后对补全的数据集用 PCA。可以看出这个流程需要一个训练数据集和一个对照集（holdout set）；管线命令会让事情更简单，不过之前我们做一些准备工作。</p><p>首先加载需要的模块：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> pipeline<span class="token punctuation">,</span> preprocessing<span class="token punctuation">,</span> decomposition</pre></td></tr></table></figure><p>然后，建立 <code>Imputer</code> 和 <code>PCA</code> 类：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>pca <span class="token operator">=</span> decomposition<span class="token punctuation">.</span>PCA<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>imputer <span class="token operator">=</span> preprocessing<span class="token punctuation">.</span>Imputer<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>有了两个类之后，我们就可以用管线命令处理：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>pipe <span class="token operator">=</span> pipeline<span class="token punctuation">.</span>Pipeline<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'imputer'</span><span class="token punctuation">,</span> imputer<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">'pca'</span><span class="token punctuation">,</span> pca<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>np<span class="token punctuation">.</span>set_printoptions<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>iris_data_transformed <span class="token operator">=</span> pipe<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris_data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>iris_data_transformed<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[-2.44, -0.79, -0.12, -0.1 ],
       [-2.67,  0.2 , -0.21,  0.15],
       [-2.83,  0.31, -0.19, -0.08],
       [-2.35,  0.66,  0.67, -0.06],
       [-2.68, -0.06, -0.2 , -0.4 ]])
</code></pre><p>如果我们用单独的步骤分别处理，每个步骤都要用一次 <code>fit_transform</code> ，而这里只需要用一次，而且只需要一个对象。</p><h2 id="how-it-works-14"><a class="anchor" href="#how-it-works-14">#</a> How it works...</h2><p>管线命令的每个步骤都是用一个元组表示，元组的第一个元素是对象的名称，第二个元素是对象。</p><p>本质上，这些步骤都是在管线命令调用时依次执行 <code>fit_transform</code> 方法。还有一种快速但不太简洁的管线命令建立方法，就像我们快速建立标准化调整模型一样，只不过用 <code>StandardScaler</code> 会获得更多功能。 <code>pipeline</code> 函数将自动创建管线命令的名称：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>pipe2 <span class="token operator">=</span> pipeline<span class="token punctuation">.</span>make_pipeline<span class="token punctuation">(</span>imputer<span class="token punctuation">,</span> pca<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>pipe2<span class="token punctuation">.</span>steps</pre></td></tr></table></figure><pre><code>[('imputer',
  Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)),
 ('pca', PCA(copy=True, n_components=None, whiten=False))]
</code></pre><p>这和前面的模型结果一样：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>iris_data_transformed2 <span class="token operator">=</span> pipe2<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris_data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>iris_data_transformed2<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[-2.44, -0.79, -0.12, -0.1 ],
       [-2.67,  0.2 , -0.21,  0.15],
       [-2.83,  0.31, -0.19, -0.08],
       [-2.35,  0.66,  0.67, -0.06],
       [-2.68, -0.06, -0.2 , -0.4 ]])
</code></pre><h2 id="theres-more-9"><a class="anchor" href="#theres-more-9">#</a> There's more...</h2><p>管线命令连接内部每个对象的属性是通过 <code>set_params</code> 方法实现，其参数用 <code>&lt;对象名称&gt;__&lt;对象参数&gt;</code> 表示。例如，我们设置 PCA 的主成份数量为 2：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>pipe2<span class="token punctuation">.</span>set_params<span class="token punctuation">(</span>pca__n_components<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>Pipeline(steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)), ('pca', PCA(copy=True, n_components=2, whiten=False))])
</code></pre><blockquote><p><code>__</code> 标识在 Python 社区读作<strong> dunder</strong>。</p></blockquote><p>这里 <code>n_components=2</code> 是 <code>pca</code> 本身的参数。我们再演示一下，输出将是一个<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>×</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">N \times 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.76666em;vertical-align:-.08333em"></span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">2</span></span></span></span> 维矩阵：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>iris_data_transformed3 <span class="token operator">=</span> pipe2<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>iris_data<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>iris_data_transformed3<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[-2.44, -0.79],
       [-2.67,  0.2 ],
       [-2.83,  0.31],
       [-2.35,  0.66],
       [-2.68, -0.06]])
</code></pre><h1 id="用正态随机过程处理回归"><a class="anchor" href="#用正态随机过程处理回归">#</a> 用正态随机过程处理回归</h1><p>这个主题将介绍如何用正态随机过程（Gaussian process，GP）处理回归问题。在线性模型部分，我们曾经见过在变量间可能存在相关性时，如何用贝叶斯岭回归 (Bayesian Ridge Regression) 表示先验概率分布（prior）信息。</p><p>正态分布过程关心的是方程而不是均值。但是，如果我们假设一个正态分布的均值为 0，那么我们需要确定协方差。</p><p>这样处理就与线性回归问题中先验概率分布可以用相关系数表示的情况类似。用 GP 处理的先验就可以用数据、样本数据间协方差构成函数表示，因此必须从数据中拟合得出。具体内容参考<span class="exturl" data-url="aHR0cDovL3d3dy5nYXVzc2lhbnByb2Nlc3Mub3JnLw=="> The Gaussian Processes Web Site</span>。</p><h2 id="getting-ready-15"><a class="anchor" href="#getting-ready-15">#</a> Getting ready</h2><p>首先要我们用一些数据来演示用 scikit-learn 处理 GP：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> load_boston</pre></td></tr><tr><td data-num="3"></td><td><pre>boston <span class="token operator">=</span> load_boston<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>boston_X <span class="token operator">=</span> boston<span class="token punctuation">.</span>data</pre></td></tr><tr><td data-num="5"></td><td><pre>boston_y <span class="token operator">=</span> boston<span class="token punctuation">.</span>target</pre></td></tr><tr><td data-num="6"></td><td><pre>train_set <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>boston_y<span class="token punctuation">)</span><span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">.75</span><span class="token punctuation">,</span> <span class="token number">.25</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><h2 id="how-to-do-it-15"><a class="anchor" href="#how-to-do-it-15">#</a> How to do it...</h2><p>有了数据之后，我们就创建 scikit-learn 的 <code>GaussianProcess</code> 对象。默认情况下，它使用一个常系数回归方程（constant regression function）和平方指数相关函数（ squared exponential correlation），是最主流的选择之一：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>gaussian_process <span class="token keyword">import</span> GaussianProcess</pre></td></tr><tr><td data-num="2"></td><td><pre>gp <span class="token operator">=</span> GaussianProcess<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>gp<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>boston_X<span class="token punctuation">[</span>train_set<span class="token punctuation">]</span><span class="token punctuation">,</span> boston_y<span class="token punctuation">[</span>train_set<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>GaussianProcess(beta0=None,
        corr=&lt;function squared_exponential at 0x0000000006F1C950&gt;,
        normalize=True, nugget=array(2.220446049250313e-15),
        optimizer='fmin_cobyla', random_start=1,
        random_state=&lt;mtrand.RandomState object at 0x00000000052A4BA8&gt;,
        regr=&lt;function constant at 0x0000000006F15950&gt;,
        storage_mode='full', theta0=array([[ 0.1]]), thetaL=None,
        thetaU=None, verbose=False)
</code></pre><p>其中，</p><ul><li><code>beta0</code> ：回归权重。默认是用 MLE（最大似然估计，Maximum Likelihood Estimation）方法进行估计。</li><li><code>corr</code> ：相关系数方程。提供了若干种方程，后面会介绍。</li><li><code>normalize</code> ：默认是 <code>True</code> ，中性化调整样本值，方便应用 MLE 进行估计。</li><li><code>nugget</code> ：正则化参数，是可选的，默认是一个很小的值。你可以将这个参数用于每个样本值（参数是一个数值），也可以对样本值使用不同的参数（参数是一个数组，与样本值个数相等）。</li><li><code>regr</code> ：默认是常系数回归方程。</li></ul><p>现在让我们拟合对象看看测试效果：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>test_preds <span class="token operator">=</span> gp<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>boston_X<span class="token punctuation">[</span><span class="token operator">~</span>train_set<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><p>让我们把预测值和实际值画出来比较一下。因为我们做了回归，还可以看看残差散点图和残差直方图。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="3"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nrows<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>f<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_preds<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> test_preds<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Predicted Values'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_preds<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> boston_y<span class="token punctuation">[</span><span class="token operator">~</span>train_set<span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Actual Values'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="7"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Predicted vs Actuals"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="8"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'best'</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="9"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>test_preds<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span></pre></td></tr><tr><td data-num="10"></td><td><pre>test_preds <span class="token operator">-</span> boston_y<span class="token punctuation">[</span><span class="token operator">~</span>train_set<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="11"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Plotted Residuals"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="12"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>hist<span class="token punctuation">(</span>test_preds <span class="token operator">-</span> boston_y<span class="token punctuation">[</span><span class="token operator">~</span>train_set<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="13"></td><td><pre>ax<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Histogram of Residuals"</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-15-1.png" alt="png"></p><h2 id="how-it-works-15"><a class="anchor" href="#how-it-works-15">#</a> How it works...</h2><p>上面我们快速演示了一下，现在让我们看看这些参数，看看如何优化它们。首先，我们看看 <code>corr</code> 函数的类型。这个函数描述了不同组 <code>X</code> 之间的相关性。scikit-learn 提供了 5 种函数类型：</p><ul><li>绝对值指数函数（absolute_exponential）</li><li>平方指数函数（squared_exponential）</li><li>广义指数函数（generalized_exponential）</li><li>立方项函数（cubic）</li><li>线性函数（linear）</li></ul><p>例如，平方指数函数公式如下：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo>−</mo><mfrac><mrow><mi mathvariant="normal">∣</mi><mi>d</mi><msup><mi mathvariant="normal">∣</mi><mn>2</mn></msup></mrow><mrow><mn>2</mn><msup><mi>l</mi><mn>2</mn></msup></mrow></mfrac></mrow></mrow><annotation encoding="application/x-tex">K= \exp {-\frac {|d|^2}{2l^2}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.177108em;vertical-align:-.686em"></span><span class="mop">exp</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.491108em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span><span class="mord"><span class="mord mathnormal" style="margin-right:.01968em">l</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.740108em"><span style="top:-2.9890000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">∣</span><span class="mord mathnormal">d</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p><p>另外，线性函数就是两个点的点积：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mo>=</mo><msup><mi>x</mi><mi>T</mi></msup><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">K=x^T{x&#x27;}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.8913309999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8913309999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.801892em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></p><p>另一个参数是 <code>theta0</code> ，表示参数估计的起始点。</p><p>一旦我们有了<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.68333em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.07153em">K</span></span></span></span> 和均值的估计值，过程就完全确定了，因为它是一个 GP；之所以用正态分布，是因为在机器学习中它一直很受欢迎。</p><p>下面我们换个 <code>regr</code> 函数类型和 <code>theta0</code> 参数，看看结果会如何变化：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>gp <span class="token operator">=</span> GaussianProcess<span class="token punctuation">(</span>regr<span class="token operator">=</span><span class="token string">'linear'</span><span class="token punctuation">,</span> theta0<span class="token operator">=</span><span class="token number">5e-1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>gp<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>boston_X<span class="token punctuation">[</span>train_set<span class="token punctuation">]</span><span class="token punctuation">,</span> boston_y<span class="token punctuation">[</span>train_set<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="3"></td><td><pre>linear_preds <span class="token operator">=</span> gp<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>boston_X<span class="token punctuation">[</span><span class="token operator">~</span>train_set<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>f<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre>ax<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>test_preds <span class="token operator">-</span> boston_y<span class="token punctuation">[</span><span class="token operator">~</span>train_set<span class="token punctuation">]</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'Residuals Original'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">.5</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="4"></td><td><pre>ax<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>linear_preds <span class="token operator">-</span> boston_y<span class="token punctuation">[</span><span class="token operator">~</span>train_set<span class="token punctuation">]</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'Residuals Linear'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'r'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">.5</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Residuals"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'best'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-15-2.png" alt="png"></p><p>很明显，第二个模型的预测效果大部分区间要更好。如果我们把残差汇总起来，我们可以看看 MSE 预测的结果：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>test_preds <span class="token operator">-</span> boston_y<span class="token punctuation">[</span><span class="token operator">~</span>train_set<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>17.456331927446904
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>np<span class="token punctuation">.</span>power<span class="token punctuation">(</span>linear_preds <span class="token operator">-</span> boston_y<span class="token punctuation">[</span><span class="token operator">~</span>train_set<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>9.320038747573518
</code></pre><h2 id="theres-more-10"><a class="anchor" href="#theres-more-10">#</a> There's more...</h2><p>我们可能还想掌握估计的不确定性。在我们预测的时候，如果我们 <code>eval_MSE</code> 设置为 <code>True</code> ，我们就获得 MSE 的值，这时预测返回的是预测值与 MSE 估计值的元组。</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>test_preds<span class="token punctuation">,</span> MSE <span class="token operator">=</span> gp<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>boston_X<span class="token punctuation">[</span><span class="token operator">~</span>train_set<span class="token punctuation">]</span><span class="token punctuation">,</span> eval_MSE<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>MSE<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([ 5.14026315,  5.30852052,  0.91152632,  2.87148688,  2.55714482])
</code></pre><p>这样我们就可以计算估计的误差了。让我们画出来看看准确率：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="2"></td><td><pre>n <span class="token operator">=</span> <span class="token number">120</span></pre></td></tr><tr><td data-num="3"></td><td><pre>rng <span class="token operator">=</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>ax<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>rng<span class="token punctuation">,</span> test_preds<span class="token punctuation">[</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax<span class="token punctuation">.</span>errorbar<span class="token punctuation">(</span>rng<span class="token punctuation">,</span> test_preds<span class="token punctuation">[</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">,</span> yerr<span class="token operator">=</span><span class="token number">1.96</span><span class="token operator">*</span>MSE<span class="token punctuation">[</span><span class="token punctuation">:</span>n<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Predictions with Error Bars"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>ax<span class="token punctuation">.</span>set_xlim<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">140</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-15-3.png" alt="png"></p><p>你会看到，许多点的估计都有些变化。但是，前面的数据显示，总体误差不是特别大。</p><h1 id="直接定义一个正态随机过程对象"><a class="anchor" href="#直接定义一个正态随机过程对象">#</a> 直接定义一个正态随机过程对象</h1><p>前面我们只触及了正态随机过程的表面。在本主题中，我们将介绍直接创建一个具有指定相关函数的正态随机过程。</p><h2 id="getting-ready-16"><a class="anchor" href="#getting-ready-16">#</a> Getting ready</h2><p><code>gaussian_process</code> 模块可以直接连接不同的相关函数与回归方程。这样就可以不创建 <code>GaussianProcess</code> 对象，直接通过函数创建需要的对象。如果你更熟悉面向对象的编程方法，这里只算是模块级的一个类方法而已。</p><p>在本主题中，我们将使用大部分函数，并把他们的结果用几个例子显示出来。如果你想真正掌握这些相关函数的特点，不要仅仅停留在这些例子上。这里不再介绍新的数学理论，让我们直接演示如何做。</p><h2 id="how-to-do-it-16"><a class="anchor" href="#how-to-do-it-16">#</a> How to do it...</h2><p>首先，我们导入要回归的数据：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> make_regression</pre></td></tr><tr><td data-num="2"></td><td><pre>X<span class="token punctuation">,</span> y <span class="token operator">=</span> make_regression<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>gaussian_process <span class="token keyword">import</span> regression_models</pre></td></tr></table></figure><p>第一个相关函数是常系数相关函数。它有若干常数构成：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>regression_models<span class="token punctuation">.</span>constant<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 1.],
       [ 1.],
       [ 1.],
       [ 1.],
       [ 1.]])
</code></pre><p>还有线性相关函数与平方指数相关函数，它们也是 <code>GaussianProcess</code> 类的默认值：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>regression_models<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 1.        , -1.29786999]])
</code></pre><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>regression_models<span class="token punctuation">.</span>quadratic<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span></pre></td></tr></table></figure><pre><code>array([[ 1.        , -1.29786999,  1.68446652]])
</code></pre><h2 id="how-it-works-16"><a class="anchor" href="#how-it-works-16">#</a> How it works...</h2><p>这样我们就可以得到回归函数了，可以直接用 <code>GaussianProcess</code> 对象来处理它们。默认值是常系数相关函数，但我们也可以把轻松的把线性模型和平方指数模型传递进去。</p><h1 id="用随机梯度下降处理回归"><a class="anchor" href="#用随机梯度下降处理回归">#</a> 用随机梯度下降处理回归</h1><p>本主题将介绍随机梯度下降法（Stochastic Gradient Descent，SGD），我们将用它解决回归问题，后面我们还用它处理分类问题。</p><h2 id="getting-ready-17"><a class="anchor" href="#getting-ready-17">#</a> Getting ready</h2><p>SGD 是机器学习中的无名英雄（unsung hero），许多算法的底层都有 SGD 的身影。之所以受欢迎是因为其简便与快速 —— 处理大量数据时这些都是好事儿。</p><p>SGD 成为许多机器学习算法的核心的另一个原因是它很容易描述过程。在本章的最后，我们对数据作一些变换，然后用模型的损失函数（loss function）拟合数据。</p><h2 id="how-to-do-it-17"><a class="anchor" href="#how-to-do-it-17">#</a> How to do it...</h2><p>如果 SGD 适合处理大数据集，我们就用大点儿的数据集来演示：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets</pre></td></tr><tr><td data-num="2"></td><td><pre>X<span class="token punctuation">,</span> y <span class="token operator">=</span> datasets<span class="token punctuation">.</span>make_regression<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">1e6</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="3"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"&#123;:,&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span><span class="token number">1e6</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>1,000,000
</code></pre><p>值得进一步了解数据对象的构成和规模信息。还在我们用的是 NumPy 数组，所以我们可以获得 <code>nbytes</code> 。Python 本身没有获取 NumPy 数组大小的方法。输出结果与系统有关，你的结果和下面的数据可能不同：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"&#123;:,&#125;"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>X<span class="token punctuation">.</span>nbytes<span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>800,000,000
</code></pre><p>我们把字节码 <code>nbytes</code> 转换成 MB（megabytes），看着更直观：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X<span class="token punctuation">.</span>nbytes <span class="token operator">/</span> <span class="token number">1e6</span></pre></td></tr></table></figure><pre><code>800.0
</code></pre><p>因此，每个数据点的字节数就是：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>X<span class="token punctuation">.</span>nbytes <span class="token operator">/</span> <span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>8.0
</code></pre><p>这些信息和我们的目标没多大关系，不过了解数据对象的构成和规模信息还是值得的。</p><p>现在，我们有了数据，就用 <code>SGDRegressor</code> 来拟合：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> linear_model</pre></td></tr><tr><td data-num="3"></td><td><pre>sgd <span class="token operator">=</span> linear_model<span class="token punctuation">.</span>SGDRegressor<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>train <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">,</span> p<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">.75</span><span class="token punctuation">,</span> <span class="token number">.25</span><span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>sgd<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">[</span>train<span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">[</span>train<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><pre><code>SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,
       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',
       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,
       random_state=None, shuffle=True, verbose=0, warm_start=False)
</code></pre><p>这里又出现一个 “充实的（beefy）” 对象。重点需要了解我们的损失函数是 <code>squared_loss</code> ，与线性回归里的残差平方和是一样的。还需要注意 <code>shuffle</code> 会对数据产生随机搅动（shuffle），这在解决伪相关问题时很有用。scikit-learn 用 <code>fit_intercept</code> 方法可以自动加一列 1。如果想从拟合结果中看到很多输出，就把 <code>verbose</code> 设为 1。用 scikit-learn 的 API 预测，我们可以统计残差的分布情况：</p><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre>linear_preds <span class="token operator">=</span> sgd<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token operator">~</span>train<span class="token punctuation">]</span><span class="token punctuation">)</span></pre></td></tr></table></figure><figure class="highlight python"><figcaption data-lang="python"></figcaption><table><tr><td data-num="1"></td><td><pre><span class="token operator">%</span>matplotlib inline</pre></td></tr><tr><td data-num="2"></td><td><pre><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt</pre></td></tr><tr><td data-num="3"></td><td><pre>f<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="4"></td><td><pre>f<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="5"></td><td><pre>ax<span class="token punctuation">.</span>hist<span class="token punctuation">(</span>linear_preds <span class="token operator">-</span> y<span class="token punctuation">[</span><span class="token operator">~</span>train<span class="token punctuation">]</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">'Residuals Linear'</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">.5</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr><tr><td data-num="6"></td><td><pre>ax<span class="token punctuation">.</span>set_title<span class="token punctuation">(</span><span class="token string">"Residuals"</span><span class="token punctuation">)</span></pre></td></tr><tr><td data-num="7"></td><td><pre>ax<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'best'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></pre></td></tr></table></figure><p><img data-src="/assets/computer-science/machine-learing/Scikit-learn-Cookbook/1-17-1.png" alt="png"></p><p>拟合的效果非常好。异常值很少，直方图也呈现出完美的正态分布钟形图。</p><h2 id="how-it-works-17"><a class="anchor" href="#how-it-works-17">#</a> How it works...</h2><p>当然这里我们用的是虚拟数据集，但是你也可以用更大的数据集合。例如，如果你在华尔街工作，有可能每天一个市场都有 20 亿条交易数据。现在如果有一周或一年的数据，用 SGD 算法就可能无法运行了。很难处理这么大的数据量，因为标准的梯度下降法每一步都要计算梯度，计算量非常庞大。</p><p>标准的梯度下降法的思想是在每次迭代计算一个新的相关系数矩阵，然后用学习速率（learning rate）和目标函数（objective function）的梯度调整它，直到相关系数矩阵收敛为止。如果用伪代码写就是这样：</p><pre><code>while not_converged:
    w = w – learning_rate * gradient(cost(w))
</code></pre><p>这里涉及的变量包括：</p><ul><li><code>w</code> ：相关系数矩阵</li><li><code>learning_rate</code> ：每次迭代时前进的长度。如果收敛效果不好，调整这个参数很重要</li><li><code>gradient</code> ：导数矩阵</li><li><code>cost</code> ：回归的残差平方和。后面我们会介绍，不同的分类方法中损失函数定义不同，具有可变性也是 SGD 应用广泛的理由之一。</li></ul><p>除了梯度函数有点复杂之外，这个方法还是可以的。随着相关系数向量的增加，梯度的计算也会变得越来越慢。每次更新之前，我们都需要对每个数据点计算新权重。</p><p>SGD 的工作方式稍有不同；每次迭代不是批量更新梯度，而是只更新新数据点的参数。这些数据点是随机选择的，因此称为随机梯度下降法。</p><div class="tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="ic i-tag"></i> 机器学习</a> <a href="/tags/Scikit-learn-Cookbook/" rel="tag"><i class="ic i-tag"></i> Scikit-learn Cookbook</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2021-12-03 21:28:21" itemprop="dateModified" datetime="2021-12-03T21:28:21+08:00">2021-12-03</time> </span><span id="computer-science/machine-learing/Scikit-learn-Cookbook/第1章 模型预处理/" class="item leancloud_visitors" data-flag-title="第 1 章 模型预处理" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.jpg" alt="Hazuki 叶月 微信支付"><p>微信支付</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>Hazuki 叶月 <i class="ic i-at"><em>@</em></i></li><li class="link"><strong>本文链接：</strong> <a href="https://hazuki.cn/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC1%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E5%A4%84%E7%90%86/" title="第 1 章 模型预处理">https://hazuki.cn/computer-science/machine-learing/Scikit-learn-Cookbook/第1章 模型预处理/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC2%E7%AB%A0%20%E5%A4%84%E7%90%86%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;tva4.sinaimg.cn&#x2F;large&#x2F;deef1a0cly8gy0yhew5ivj21e80u07g2.jpg" title="第2章 处理线性模型"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> Scikit-learn Cookbook</span><h3>第2章 处理线性模型</h3></a></div><div class="item right"><a href="/computer-science/computer-network/transport-layer/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;tva2.sinaimg.cn&#x2F;large&#x2F;deef1a0cly8gy0wt2eo0lj21hc0u0qb4.jpg" title="第3章 传输层"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 计算机网络</span><h3>第3章 传输层</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">第一章 模型预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">简介</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%8E%E5%A4%96%E9%83%A8%E6%BA%90%E8%8E%B7%E5%8F%96%E6%A0%B7%E6%9C%AC%E6%95%B0%E6%8D%AE"><span class="toc-number">2.</span> <span class="toc-text">从外部源获取样本数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready"><span class="toc-number">2.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it"><span class="toc-number">2.2.</span> <span class="toc-text">How to do it…</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works"><span class="toc-number">2.3.</span> <span class="toc-text">How it works…</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more"><span class="toc-number">2.4.</span> <span class="toc-text">There&#39;s more…</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#see-also"><span class="toc-number">2.5.</span> <span class="toc-text">See also</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%AF%95%E9%AA%8C%E6%A0%B7%E6%9C%AC%E6%95%B0%E6%8D%AE"><span class="toc-number">3.</span> <span class="toc-text">创建试验样本数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-2"><span class="toc-number">3.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-2"><span class="toc-number">3.2.</span> <span class="toc-text">How to do it...</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.2.1.</span> <span class="toc-text">回归数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.2.2.</span> <span class="toc-text">分类数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.2.3.</span> <span class="toc-text">聚类数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-2"><span class="toc-number">3.3.</span> <span class="toc-text">How it works...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8A%8A%E6%95%B0%E6%8D%AE%E8%B0%83%E6%95%B4%E4%B8%BA%E6%A0%87%E5%87%86%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83"><span class="toc-number">4.</span> <span class="toc-text">把数据调整为标准正态分布</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-3"><span class="toc-number">4.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-3"><span class="toc-number">4.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-3"><span class="toc-number">4.3.</span> <span class="toc-text">How it works...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-2"><span class="toc-number">4.4.</span> <span class="toc-text">There&#39;s more...</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E5%B9%82%E7%AD%89%E6%A0%87%E5%87%86%E5%8C%96idempotent-scaler%E5%AF%B9%E8%B1%A1"><span class="toc-number">4.4.1.</span> <span class="toc-text">创建幂等标准化（idempotent scaler）对象</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E7%A8%80%E7%96%8F%E6%95%B0%E6%8D%AE%E5%A1%AB%E8%A1%A5"><span class="toc-number">4.4.2.</span> <span class="toc-text">处理稀疏数据填补</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E9%98%88%E5%80%BC%E5%88%9B%E5%BB%BA%E4%BA%8C%E5%85%83%E7%89%B9%E5%BE%81"><span class="toc-number">5.</span> <span class="toc-text">用阈值创建二元特征</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-4"><span class="toc-number">5.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-4"><span class="toc-number">5.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-4"><span class="toc-number">5.3.</span> <span class="toc-text">How it works...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-3"><span class="toc-number">5.4.</span> <span class="toc-text">There&#39;s more...</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A8%80%E7%96%8F%E7%9F%A9%E9%98%B5"><span class="toc-number">5.4.1.</span> <span class="toc-text">稀疏矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fit%E6%96%B9%E6%B3%95"><span class="toc-number">5.4.2.</span> <span class="toc-text">fit 方法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F%E5%A4%84%E7%90%86"><span class="toc-number">6.</span> <span class="toc-text">分类变量处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-5"><span class="toc-number">6.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-5"><span class="toc-number">6.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-5"><span class="toc-number">6.3.</span> <span class="toc-text">How it works...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-4"><span class="toc-number">6.4.</span> <span class="toc-text">There&#39;s more...</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#dictvectorizer"><span class="toc-number">6.4.1.</span> <span class="toc-text">DictVectorizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pasty"><span class="toc-number">6.4.2.</span> <span class="toc-text">Pasty</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A0%87%E7%AD%BE%E7%89%B9%E5%BE%81%E4%BA%8C%E5%85%83%E5%8C%96"><span class="toc-number">7.</span> <span class="toc-text">标签特征二元化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-6"><span class="toc-number">7.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-6"><span class="toc-number">7.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-6"><span class="toc-number">7.3.</span> <span class="toc-text">How it works...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-5"><span class="toc-number">7.4.</span> <span class="toc-text">There&#39;s more...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="toc-number">8.</span> <span class="toc-text">处理缺失值</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-7"><span class="toc-number">8.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-7"><span class="toc-number">8.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-7"><span class="toc-number">8.3.</span> <span class="toc-text">How it works...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-6"><span class="toc-number">8.4.</span> <span class="toc-text">There&#39;s more...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E7%AE%A1%E7%BA%BF%E5%91%BD%E4%BB%A4%E5%A4%84%E7%90%86%E5%A4%9A%E4%B8%AA%E6%AD%A5%E9%AA%A4"><span class="toc-number">9.</span> <span class="toc-text">用管线命令处理多个步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-8"><span class="toc-number">9.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-8"><span class="toc-number">9.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-8"><span class="toc-number">9.3.</span> <span class="toc-text">How it works...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E9%99%8D%E7%BB%B4"><span class="toc-number">10.</span> <span class="toc-text">用主成分分析降维</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-9"><span class="toc-number">10.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-9"><span class="toc-number">10.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-9"><span class="toc-number">10.3.</span> <span class="toc-text">How it works...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-7"><span class="toc-number">10.4.</span> <span class="toc-text">There&#39;s more...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90%E9%99%8D%E7%BB%B4"><span class="toc-number">11.</span> <span class="toc-text">用因子分析降维</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-10"><span class="toc-number">11.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-10"><span class="toc-number">11.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-10"><span class="toc-number">11.3.</span> <span class="toc-text">How it works...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E6%A0%B8pca%E5%AE%9E%E7%8E%B0%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4"><span class="toc-number">12.</span> <span class="toc-text">用核 PCA 实现非线性降维</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-11"><span class="toc-number">12.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-11"><span class="toc-number">12.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-11"><span class="toc-number">12.3.</span> <span class="toc-text">How it works...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E6%88%AA%E6%96%AD%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E9%99%8D%E7%BB%B4"><span class="toc-number">13.</span> <span class="toc-text">用截断奇异值分解降维</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-12"><span class="toc-number">13.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-12"><span class="toc-number">13.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-12"><span class="toc-number">13.3.</span> <span class="toc-text">How it works...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-8"><span class="toc-number">13.4.</span> <span class="toc-text">There&#39;s more...</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7%E7%BF%BB%E8%BD%ACsign-flipping"><span class="toc-number">13.4.1.</span> <span class="toc-text">符号翻转（Sign flipping）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A8%80%E7%96%8F%E7%9F%A9%E9%98%B5-2"><span class="toc-number">13.4.2.</span> <span class="toc-text">稀疏矩阵</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E5%AD%97%E5%85%B8%E5%AD%A6%E4%B9%A0%E5%88%86%E8%A7%A3%E6%B3%95%E5%88%86%E7%B1%BB"><span class="toc-number">14.</span> <span class="toc-text">用字典学习分解法分类</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-13"><span class="toc-number">14.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-13"><span class="toc-number">14.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-13"><span class="toc-number">14.3.</span> <span class="toc-text">How it works...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E7%AE%A1%E7%BA%BF%E5%91%BD%E4%BB%A4%E8%BF%9E%E6%8E%A5%E5%A4%9A%E4%B8%AA%E8%BD%AC%E6%8D%A2%E6%96%B9%E6%B3%95"><span class="toc-number">15.</span> <span class="toc-text">用管线命令连接多个转换方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-14"><span class="toc-number">15.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-14"><span class="toc-number">15.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-14"><span class="toc-number">15.3.</span> <span class="toc-text">How it works...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-9"><span class="toc-number">15.4.</span> <span class="toc-text">There&#39;s more...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E6%AD%A3%E6%80%81%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E5%A4%84%E7%90%86%E5%9B%9E%E5%BD%92"><span class="toc-number">16.</span> <span class="toc-text">用正态随机过程处理回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-15"><span class="toc-number">16.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-15"><span class="toc-number">16.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-15"><span class="toc-number">16.3.</span> <span class="toc-text">How it works...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#theres-more-10"><span class="toc-number">16.4.</span> <span class="toc-text">There&#39;s more...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%B4%E6%8E%A5%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%E6%AD%A3%E6%80%81%E9%9A%8F%E6%9C%BA%E8%BF%87%E7%A8%8B%E5%AF%B9%E8%B1%A1"><span class="toc-number">17.</span> <span class="toc-text">直接定义一个正态随机过程对象</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-16"><span class="toc-number">17.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-16"><span class="toc-number">17.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-16"><span class="toc-number">17.3.</span> <span class="toc-text">How it works...</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%A4%84%E7%90%86%E5%9B%9E%E5%BD%92"><span class="toc-number">18.</span> <span class="toc-text">用随机梯度下降处理回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#getting-ready-17"><span class="toc-number">18.1.</span> <span class="toc-text">Getting ready</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-to-do-it-17"><span class="toc-number">18.2.</span> <span class="toc-text">How to do it...</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#how-it-works-17"><span class="toc-number">18.3.</span> <span class="toc-text">How it works...</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li class="active"><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC1%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E5%A4%84%E7%90%86/" rel="bookmark" title="第1章 模型预处理">第1章 模型预处理</a></li><li><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC2%E7%AB%A0%20%E5%A4%84%E7%90%86%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" rel="bookmark" title="第2章 处理线性模型">第2章 处理线性模型</a></li><li><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC4%E7%AB%A0%20%E4%BD%BF%E7%94%A8%20scikit-learn%20%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%88%86%E7%B1%BB/" rel="bookmark" title="第4章 使用 scikit-learn 对数据分类">第4章 使用 scikit-learn 对数据分类</a></li><li><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC5%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E5%90%8E%E5%A4%84%E7%90%86/" rel="bookmark" title="第5章 模型后处理">第5章 模型后处理</a></li><li><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC3%E7%AB%A0%20%E4%BD%BF%E7%94%A8%E8%B7%9D%E7%A6%BB%E5%90%91%E9%87%8F%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B/" rel="bookmark" title="第3章 使用距离向量构建模型">第3章 使用距离向量构建模型</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Hazuki 叶月" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Hazuki 叶月</p><div class="description" itemprop="description">计算机基础 & 编程笔记</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">11</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">8</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">9</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL0hhenVraS0yOTU=" title="https:&#x2F;&#x2F;github.com&#x2F;Hazuki-295"><i class="ic i-github"></i></span> <span class="exturl item zhihu" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3Blb3BsZS9sdW8tc2hlbmctNzYtMjItMzg=" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;luo-sheng-76-22-38"><i class="ic i-zhihu"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly95Lm11c2ljLjE2My5jb20vbS91c2VyP2lkPTMwOTUzOTk5NA==" title="https:&#x2F;&#x2F;y.music.163.com&#x2F;m&#x2F;user?id&#x3D;309539994"><i class="ic i-cloud-music"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-magic"></i>链环</a><ul class="submenu"><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>友達</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC2%E7%AB%A0%20%E5%A4%84%E7%90%86%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/computer-science/computer-network/transport-layer/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" title="分类于 Scikit-learn Cookbook">Scikit-learn Cookbook</a></div><span><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC4%E7%AB%A0%20%E4%BD%BF%E7%94%A8%20scikit-learn%20%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%88%86%E7%B1%BB/" title="第4章 使用 scikit-learn 对数据分类">第4章 使用 scikit-learn 对数据分类</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/computer-network/" title="分类于 计算机网络">计算机网络</a></div><span><a href="/computer-science/computer-network/application-layer/" title="第2章 应用层">第2章 应用层</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" title="分类于 Scikit-learn Cookbook">Scikit-learn Cookbook</a></div><span><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC5%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E5%90%8E%E5%A4%84%E7%90%86/" title="第5章 模型后处理">第5章 模型后处理</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/computer-organization/" title="分类于 计算机组成原理">计算机组成原理</a></div><span><a href="/computer-science/computer-organization/Chapter1-Computer-Abstractions-and-Technology/" title="Chapter 1 Computer Abstractions and Technology">Chapter 1 Computer Abstractions and Technology</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/computer-network/" title="分类于 计算机网络">计算机网络</a></div><span><a href="/computer-science/computer-network/network-layer-1/" title="第4章 网络层：数据平面">第4章 网络层：数据平面</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/computer-network/" title="分类于 计算机网络">计算机网络</a></div><span><a href="/computer-science/computer-network/transport-layer/" title="第3章 传输层">第3章 传输层</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" title="分类于 Scikit-learn Cookbook">Scikit-learn Cookbook</a></div><span><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC3%E7%AB%A0%20%E4%BD%BF%E7%94%A8%E8%B7%9D%E7%A6%BB%E5%90%91%E9%87%8F%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B/" title="第3章 使用距离向量构建模型">第3章 使用距离向量构建模型</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" title="分类于 Scikit-learn Cookbook">Scikit-learn Cookbook</a></div><span><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC2%E7%AB%A0%20%E5%A4%84%E7%90%86%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" title="第2章 处理线性模型">第2章 处理线性模型</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/operating-system/" title="分类于 操作系统">操作系统</a></div><span><a href="/computer-science/operating-system/Chapter1-Introduction/" title="Chapter 1 Introduction">Chapter 1 Introduction</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/computer-science/" title="分类于 计算机科学">计算机科学</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/computer-science/machine-learing/Scikit-learn-Cookbook/" title="分类于 Scikit-learn Cookbook">Scikit-learn Cookbook</a></div><span><a href="/computer-science/machine-learing/Scikit-learn-Cookbook/%E7%AC%AC1%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E9%A2%84%E5%A4%84%E7%90%86/" title="第1章 模型预处理">第1章 模型预处理</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Hazuki 叶月 @ Hazuki の 小屋</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">178k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">2:41</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"computer-science/machine-learing/Scikit-learn-Cookbook/第1章 模型预处理/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html>